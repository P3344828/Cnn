{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DS-AG-021\n",
        "# CNN Architecture | Assignment\n",
        "\n",
        "Q1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "\n",
        "- Ans: Filters (Kernels): Filters are small, learnable weight matrices that slide over the input and perform convolution operations. Their role is to extract specific local features (e.g., edges, textures) by responding strongly to particular patterns.\n",
        "\n",
        "- Feature Maps: Feature maps are the outputs produced after applying filters to the input. They represent the spatial locations and strengths of the features detected by the corresponding filters.\n",
        "\n",
        "Q2. Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect output dimensions of feature maps?\n",
        "\n",
        "- Ans: Padding refers to adding extra pixels (typically zeros) around the border of an input feature map before applying convolution.\n",
        "   - Purpose: It preserves spatial dimensions and allows the filter to process edge pixels effectively.\n",
        "   - Effect on Output Dimensions: For an input of size (N*N), a filter of size (F*F), stride S, and padding P, the output dimension O is:\n",
        "    \n",
        "                  O = [(N - F + 2P) / S] + 1\n",
        "\n",
        "      - Increasing padding increases the output size or helps maintain it.\n",
        "\n",
        "- Stride defines the number of pixels by which the convolutional filter moves across the input feature map.\n",
        "  - Effect on Output Dimensions: A larger stride reduces the output dimension, effectively performing downsampling, as per the same formula:\n",
        "\n",
        "    \n",
        "                 O = [(N - F + 2P) / S] + 1\n",
        "\n",
        "Q3. Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "- Ans: The receptive field of a neuron in a CNN is the region of the input image that influences the neuron's activation. In other words, it is the spatial extent of the input \"seen\" by that neuron.\n",
        "\n",
        "- **Importance in Deep Architectures:**\n",
        "\n",
        "  - Hierarchical Feature Learning: Larger receptive fields in deeper layers allow neurons to capture more global and abstract features.\n",
        "\n",
        "  - Contextual Understanding: Essential for recognizing patterns that depend on wider context, such as objects in an image.\n",
        "\n",
        "  - Design Consideration: Helps in determining network depth and kernel sizes to ensure the network can capture features at the required scale.\n",
        "\n",
        "Q4. Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "- Ans: ** Influence of Filter Size and Stride on CNN Parameters:**\n",
        "\n",
        "   - Filter Size (Kernel Size):\n",
        "       - The number of learnable parameters in a convolutional layer depends directly on the filter size.\n",
        "       - For a layer with (K) filters of size (F*F) and C input channels, the total number of parameters is:\n",
        "\n",
        "          Parameters = K* (F * F * C) + K\n",
        "     \n",
        "      (the +K accounts for bias terms)\n",
        "\n",
        "       - Effect: Larger filters increase the number of parameters and model complexity.\n",
        "\n",
        "    - Stride:\n",
        "        -  It determines the step size of the filter over the input.\n",
        "        - Effect on Parameters: Stride does not change the number of learnable parameters, as it only affects the spatial size of the output feature map, not the filter itself.\n",
        "\n",
        "Q5. Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "- Ans: **LeNet (1998)**\n",
        "\n",
        "    - Depth: Shallow - 5 layers (2 conv + 3 FC)\n",
        "    - Filter Sizes: Large in early layers (5 * 5), small number of filters\n",
        "    - Stride & Padding: Stride 1, minimal padding\n",
        "    - Feature Maps/ Channels: Few filters (6-16)\n",
        "    - Performance: Good for small datasets (MNIST)\n",
        "    - Key Innovations: First practical CNN for digit recognition\n",
        "\n",
        "- **AlexNet (2012)**\n",
        "\n",
        "    - Depth: Deeper - 8 layers (5 conv + 3 FC)\n",
        "    - Filter Sizes: (11 * 11) in first layer, then (5 * 5) and (3 * 3); more filters per layer\n",
        "    - Stride & Padding: Stride 4 in first layer, padding varies\n",
        "    - Feature Maps/ Channels: Many filters (up to 384-256)\n",
        "    - Performance: Breakthrough on ImageNet, better generalization\n",
        "    - Key Innovations: ReLU activation, dropout, overlapping pooling, GPU training\n",
        "\n",
        "- **VGG (2014)**\n",
        "\n",
        "    - Depth: very deep - 16-19 layers (all conv + FC at the end)\n",
        "    - Filter Sizes: Uniform small filters (3 * 3) throughout; depth compensates for receptive field\n",
        "    - Stride & Padding: Stride 1, padding used to preserve spatial dimensions\n",
        "    - Feature Maps/ Channels: Gradually increasing filters (64 → 512)\n",
        "    - Performance: High accuracy on ImageNet, very effective for deep feature extraction\n",
        "    - Key Innovations: Very deep, uniform architecture; simplicity in design; small filters improve non-linearity\n",
        "\n",
        "Q10. You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into \"Normal\" and \"Pneumonia\" categories. Describe your end-to-end approach from-data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "- Ans:\n",
        "1. **Data Preparaton:**\n",
        "    - Collect labeled chest X-ray images for \"Normal\" and \"Pneumonia.\"\n",
        "    - Perform preprocessing: resizing, normalization, and augmentation (rotation, flipping) to enhance generalization.\n",
        "    - Split data into training, validation, and test sets.\n",
        "\n",
        "2. **Model Development:**\n",
        "   - Choose a CNN architecture (e.g., ResNet, EfficientNet, or a custom CNN).\n",
        "   - Compile the model with an appropriate loss function (binary cross-entropy)\n",
        "  and optimizer (Adam).\n",
        "   - Train the model on the training set with early stopping and monitor performance on the validation set.\n",
        "   - Evaluate final accuracy and other metrics on the test set.\n",
        "\n",
        "3. **Model Saving:**\n",
        "  - Save the trained model using formats like `.h5` or TensorFlow SavedModel for deployment.\n",
        "\n",
        "4. **Web App Deployment:**\n",
        "  - Build a Streamlit app that:\n",
        "     - Accepts X-ray image uploads from users.\n",
        "     - Loads the trained CNN model.\n",
        "     - Preprocesses the uploaded image and predicts the class.\n",
        "     - Displays the result with confidence scores.\n",
        "  - Deploy the Streamlit app on platforms like Streamlit Cloud, Heroku, or AWS.\n",
        "\n",
        "5. **Monitoring & Maintenance:**\n",
        "    - Implement logging and feedback mechanisms for model predictions.\n",
        "    - Periodically retrain the model with new data to maintain accuracy."
      ],
      "metadata": {
        "id": "_eqcrThH9OzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "# Import required modules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST and Preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN and Compile model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train & Evaluate model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.1)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5NrEF7EZHfR",
        "outputId": "0359c7bb-24a5-4e64-d77e-2e6a665220c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.8396 - loss: 0.5316 - val_accuracy: 0.9825 - val_loss: 0.0629\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9793 - loss: 0.0667 - val_accuracy: 0.9830 - val_loss: 0.0575\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9865 - loss: 0.0452 - val_accuracy: 0.9900 - val_loss: 0.0367\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9899 - loss: 0.0326 - val_accuracy: 0.9907 - val_loss: 0.0359\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9922 - loss: 0.0255 - val_accuracy: 0.9893 - val_loss: 0.0395\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9852 - loss: 0.0446\n",
            "Test Accuracy: 0.9880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load CIFAR-10 & Preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN & Compile model CIFAR-10\n",
        "model_cifar = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model_cifar.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train & Evaluate model\n",
        "history_cifar = model_cifar.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
        "test_loss_cifar, test_acc_cifar = model_cifar.evaluate(x_test, y_test)\n",
        "print(f\"CIFAR-10 Test Accuracy: {test_acc_cifar:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-epvtKzZIhq",
        "outputId": "dfd5b18b-b351-4847-caf4-6047cd822b66"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.3871 - loss: 1.6674 - val_accuracy: 0.6254 - val_loss: 1.0531\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6652 - loss: 0.9498 - val_accuracy: 0.7136 - val_loss: 0.8300\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7536 - loss: 0.7033 - val_accuracy: 0.7548 - val_loss: 0.7148\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8199 - loss: 0.5241 - val_accuracy: 0.7590 - val_loss: 0.7047\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8684 - loss: 0.3740 - val_accuracy: 0.7702 - val_loss: 0.7542\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9212 - loss: 0.2275 - val_accuracy: 0.7676 - val_loss: 0.7953\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9558 - loss: 0.1340 - val_accuracy: 0.7558 - val_loss: 1.0211\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9651 - loss: 0.0983 - val_accuracy: 0.7462 - val_loss: 1.2731\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9710 - loss: 0.0841 - val_accuracy: 0.7440 - val_loss: 1.3287\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9796 - loss: 0.0624 - val_accuracy: 0.7512 - val_loss: 1.3262\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7354 - loss: 1.3956\n",
            "CIFAR-10 Test Accuracy: 0.7350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Using PyTorch, write a script to define to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration & Transformations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "])\n",
        "\n",
        "# Load MNIST dataset & Define CNN model\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Corrected input size for the first fully connected layer\n",
        "        self.fc1 = nn.Linear(64*12*12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        # Corrected flattening operation\n",
        "        x = x.view(-1, 64*12*12)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Loss and Optimizer & Training loop\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct/total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mss7LB5PgyYb",
        "outputId": "3ab5ac3d-3c72-4aa8-b669-3ef53b0f7aac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0049\n",
            "Epoch [1/5], Loss: 0.0097\n",
            "Epoch [1/5], Loss: 0.0138\n",
            "Epoch [1/5], Loss: 0.0175\n",
            "Epoch [1/5], Loss: 0.0207\n",
            "Epoch [1/5], Loss: 0.0232\n",
            "Epoch [1/5], Loss: 0.0254\n",
            "Epoch [1/5], Loss: 0.0271\n",
            "Epoch [1/5], Loss: 0.0284\n",
            "Epoch [1/5], Loss: 0.0297\n",
            "Epoch [1/5], Loss: 0.0311\n",
            "Epoch [1/5], Loss: 0.0323\n",
            "Epoch [1/5], Loss: 0.0336\n",
            "Epoch [1/5], Loss: 0.0343\n",
            "Epoch [1/5], Loss: 0.0353\n",
            "Epoch [1/5], Loss: 0.0362\n",
            "Epoch [1/5], Loss: 0.0371\n",
            "Epoch [1/5], Loss: 0.0380\n",
            "Epoch [1/5], Loss: 0.0388\n",
            "Epoch [1/5], Loss: 0.0396\n",
            "Epoch [1/5], Loss: 0.0402\n",
            "Epoch [1/5], Loss: 0.0411\n",
            "Epoch [1/5], Loss: 0.0420\n",
            "Epoch [1/5], Loss: 0.0427\n",
            "Epoch [1/5], Loss: 0.0436\n",
            "Epoch [1/5], Loss: 0.0447\n",
            "Epoch [1/5], Loss: 0.0457\n",
            "Epoch [1/5], Loss: 0.0462\n",
            "Epoch [1/5], Loss: 0.0470\n",
            "Epoch [1/5], Loss: 0.0480\n",
            "Epoch [1/5], Loss: 0.0489\n",
            "Epoch [1/5], Loss: 0.0496\n",
            "Epoch [1/5], Loss: 0.0500\n",
            "Epoch [1/5], Loss: 0.0505\n",
            "Epoch [1/5], Loss: 0.0514\n",
            "Epoch [1/5], Loss: 0.0521\n",
            "Epoch [1/5], Loss: 0.0528\n",
            "Epoch [1/5], Loss: 0.0534\n",
            "Epoch [1/5], Loss: 0.0544\n",
            "Epoch [1/5], Loss: 0.0548\n",
            "Epoch [1/5], Loss: 0.0558\n",
            "Epoch [1/5], Loss: 0.0565\n",
            "Epoch [1/5], Loss: 0.0570\n",
            "Epoch [1/5], Loss: 0.0579\n",
            "Epoch [1/5], Loss: 0.0586\n",
            "Epoch [1/5], Loss: 0.0592\n",
            "Epoch [1/5], Loss: 0.0600\n",
            "Epoch [1/5], Loss: 0.0604\n",
            "Epoch [1/5], Loss: 0.0612\n",
            "Epoch [1/5], Loss: 0.0617\n",
            "Epoch [1/5], Loss: 0.0621\n",
            "Epoch [1/5], Loss: 0.0629\n",
            "Epoch [1/5], Loss: 0.0636\n",
            "Epoch [1/5], Loss: 0.0643\n",
            "Epoch [1/5], Loss: 0.0648\n",
            "Epoch [1/5], Loss: 0.0652\n",
            "Epoch [1/5], Loss: 0.0657\n",
            "Epoch [1/5], Loss: 0.0662\n",
            "Epoch [1/5], Loss: 0.0669\n",
            "Epoch [1/5], Loss: 0.0675\n",
            "Epoch [1/5], Loss: 0.0680\n",
            "Epoch [1/5], Loss: 0.0687\n",
            "Epoch [1/5], Loss: 0.0693\n",
            "Epoch [1/5], Loss: 0.0698\n",
            "Epoch [1/5], Loss: 0.0703\n",
            "Epoch [1/5], Loss: 0.0707\n",
            "Epoch [1/5], Loss: 0.0710\n",
            "Epoch [1/5], Loss: 0.0716\n",
            "Epoch [1/5], Loss: 0.0721\n",
            "Epoch [1/5], Loss: 0.0727\n",
            "Epoch [1/5], Loss: 0.0733\n",
            "Epoch [1/5], Loss: 0.0737\n",
            "Epoch [1/5], Loss: 0.0741\n",
            "Epoch [1/5], Loss: 0.0746\n",
            "Epoch [1/5], Loss: 0.0749\n",
            "Epoch [1/5], Loss: 0.0755\n",
            "Epoch [1/5], Loss: 0.0758\n",
            "Epoch [1/5], Loss: 0.0763\n",
            "Epoch [1/5], Loss: 0.0766\n",
            "Epoch [1/5], Loss: 0.0770\n",
            "Epoch [1/5], Loss: 0.0773\n",
            "Epoch [1/5], Loss: 0.0777\n",
            "Epoch [1/5], Loss: 0.0780\n",
            "Epoch [1/5], Loss: 0.0783\n",
            "Epoch [1/5], Loss: 0.0787\n",
            "Epoch [1/5], Loss: 0.0789\n",
            "Epoch [1/5], Loss: 0.0792\n",
            "Epoch [1/5], Loss: 0.0795\n",
            "Epoch [1/5], Loss: 0.0798\n",
            "Epoch [1/5], Loss: 0.0805\n",
            "Epoch [1/5], Loss: 0.0807\n",
            "Epoch [1/5], Loss: 0.0810\n",
            "Epoch [1/5], Loss: 0.0813\n",
            "Epoch [1/5], Loss: 0.0817\n",
            "Epoch [1/5], Loss: 0.0823\n",
            "Epoch [1/5], Loss: 0.0825\n",
            "Epoch [1/5], Loss: 0.0827\n",
            "Epoch [1/5], Loss: 0.0831\n",
            "Epoch [1/5], Loss: 0.0835\n",
            "Epoch [1/5], Loss: 0.0838\n",
            "Epoch [1/5], Loss: 0.0840\n",
            "Epoch [1/5], Loss: 0.0842\n",
            "Epoch [1/5], Loss: 0.0843\n",
            "Epoch [1/5], Loss: 0.0846\n",
            "Epoch [1/5], Loss: 0.0849\n",
            "Epoch [1/5], Loss: 0.0855\n",
            "Epoch [1/5], Loss: 0.0857\n",
            "Epoch [1/5], Loss: 0.0860\n",
            "Epoch [1/5], Loss: 0.0862\n",
            "Epoch [1/5], Loss: 0.0864\n",
            "Epoch [1/5], Loss: 0.0867\n",
            "Epoch [1/5], Loss: 0.0872\n",
            "Epoch [1/5], Loss: 0.0873\n",
            "Epoch [1/5], Loss: 0.0877\n",
            "Epoch [1/5], Loss: 0.0879\n",
            "Epoch [1/5], Loss: 0.0881\n",
            "Epoch [1/5], Loss: 0.0884\n",
            "Epoch [1/5], Loss: 0.0887\n",
            "Epoch [1/5], Loss: 0.0888\n",
            "Epoch [1/5], Loss: 0.0889\n",
            "Epoch [1/5], Loss: 0.0893\n",
            "Epoch [1/5], Loss: 0.0895\n",
            "Epoch [1/5], Loss: 0.0898\n",
            "Epoch [1/5], Loss: 0.0901\n",
            "Epoch [1/5], Loss: 0.0904\n",
            "Epoch [1/5], Loss: 0.0907\n",
            "Epoch [1/5], Loss: 0.0909\n",
            "Epoch [1/5], Loss: 0.0912\n",
            "Epoch [1/5], Loss: 0.0916\n",
            "Epoch [1/5], Loss: 0.0919\n",
            "Epoch [1/5], Loss: 0.0920\n",
            "Epoch [1/5], Loss: 0.0922\n",
            "Epoch [1/5], Loss: 0.0926\n",
            "Epoch [1/5], Loss: 0.0929\n",
            "Epoch [1/5], Loss: 0.0930\n",
            "Epoch [1/5], Loss: 0.0932\n",
            "Epoch [1/5], Loss: 0.0935\n",
            "Epoch [1/5], Loss: 0.0936\n",
            "Epoch [1/5], Loss: 0.0939\n",
            "Epoch [1/5], Loss: 0.0941\n",
            "Epoch [1/5], Loss: 0.0942\n",
            "Epoch [1/5], Loss: 0.0946\n",
            "Epoch [1/5], Loss: 0.0949\n",
            "Epoch [1/5], Loss: 0.0952\n",
            "Epoch [1/5], Loss: 0.0954\n",
            "Epoch [1/5], Loss: 0.0956\n",
            "Epoch [1/5], Loss: 0.0957\n",
            "Epoch [1/5], Loss: 0.0958\n",
            "Epoch [1/5], Loss: 0.0961\n",
            "Epoch [1/5], Loss: 0.0963\n",
            "Epoch [1/5], Loss: 0.0965\n",
            "Epoch [1/5], Loss: 0.0967\n",
            "Epoch [1/5], Loss: 0.0968\n",
            "Epoch [1/5], Loss: 0.0969\n",
            "Epoch [1/5], Loss: 0.0972\n",
            "Epoch [1/5], Loss: 0.0976\n",
            "Epoch [1/5], Loss: 0.0977\n",
            "Epoch [1/5], Loss: 0.0978\n",
            "Epoch [1/5], Loss: 0.0982\n",
            "Epoch [1/5], Loss: 0.0983\n",
            "Epoch [1/5], Loss: 0.0985\n",
            "Epoch [1/5], Loss: 0.0986\n",
            "Epoch [1/5], Loss: 0.0989\n",
            "Epoch [1/5], Loss: 0.0992\n",
            "Epoch [1/5], Loss: 0.0994\n",
            "Epoch [1/5], Loss: 0.0995\n",
            "Epoch [1/5], Loss: 0.0998\n",
            "Epoch [1/5], Loss: 0.1001\n",
            "Epoch [1/5], Loss: 0.1004\n",
            "Epoch [1/5], Loss: 0.1006\n",
            "Epoch [1/5], Loss: 0.1007\n",
            "Epoch [1/5], Loss: 0.1008\n",
            "Epoch [1/5], Loss: 0.1011\n",
            "Epoch [1/5], Loss: 0.1013\n",
            "Epoch [1/5], Loss: 0.1014\n",
            "Epoch [1/5], Loss: 0.1017\n",
            "Epoch [1/5], Loss: 0.1019\n",
            "Epoch [1/5], Loss: 0.1020\n",
            "Epoch [1/5], Loss: 0.1024\n",
            "Epoch [1/5], Loss: 0.1028\n",
            "Epoch [1/5], Loss: 0.1029\n",
            "Epoch [1/5], Loss: 0.1030\n",
            "Epoch [1/5], Loss: 0.1031\n",
            "Epoch [1/5], Loss: 0.1032\n",
            "Epoch [1/5], Loss: 0.1033\n",
            "Epoch [1/5], Loss: 0.1035\n",
            "Epoch [1/5], Loss: 0.1038\n",
            "Epoch [1/5], Loss: 0.1040\n",
            "Epoch [1/5], Loss: 0.1041\n",
            "Epoch [1/5], Loss: 0.1043\n",
            "Epoch [1/5], Loss: 0.1045\n",
            "Epoch [1/5], Loss: 0.1046\n",
            "Epoch [1/5], Loss: 0.1048\n",
            "Epoch [1/5], Loss: 0.1050\n",
            "Epoch [1/5], Loss: 0.1052\n",
            "Epoch [1/5], Loss: 0.1053\n",
            "Epoch [1/5], Loss: 0.1055\n",
            "Epoch [1/5], Loss: 0.1057\n",
            "Epoch [1/5], Loss: 0.1060\n",
            "Epoch [1/5], Loss: 0.1063\n",
            "Epoch [1/5], Loss: 0.1066\n",
            "Epoch [1/5], Loss: 0.1068\n",
            "Epoch [1/5], Loss: 0.1071\n",
            "Epoch [1/5], Loss: 0.1073\n",
            "Epoch [1/5], Loss: 0.1076\n",
            "Epoch [1/5], Loss: 0.1077\n",
            "Epoch [1/5], Loss: 0.1079\n",
            "Epoch [1/5], Loss: 0.1082\n",
            "Epoch [1/5], Loss: 0.1083\n",
            "Epoch [1/5], Loss: 0.1084\n",
            "Epoch [1/5], Loss: 0.1086\n",
            "Epoch [1/5], Loss: 0.1088\n",
            "Epoch [1/5], Loss: 0.1090\n",
            "Epoch [1/5], Loss: 0.1092\n",
            "Epoch [1/5], Loss: 0.1094\n",
            "Epoch [1/5], Loss: 0.1097\n",
            "Epoch [1/5], Loss: 0.1098\n",
            "Epoch [1/5], Loss: 0.1100\n",
            "Epoch [1/5], Loss: 0.1101\n",
            "Epoch [1/5], Loss: 0.1102\n",
            "Epoch [1/5], Loss: 0.1102\n",
            "Epoch [1/5], Loss: 0.1103\n",
            "Epoch [1/5], Loss: 0.1104\n",
            "Epoch [1/5], Loss: 0.1106\n",
            "Epoch [1/5], Loss: 0.1109\n",
            "Epoch [1/5], Loss: 0.1111\n",
            "Epoch [1/5], Loss: 0.1113\n",
            "Epoch [1/5], Loss: 0.1115\n",
            "Epoch [1/5], Loss: 0.1116\n",
            "Epoch [1/5], Loss: 0.1117\n",
            "Epoch [1/5], Loss: 0.1122\n",
            "Epoch [1/5], Loss: 0.1123\n",
            "Epoch [1/5], Loss: 0.1125\n",
            "Epoch [1/5], Loss: 0.1127\n",
            "Epoch [1/5], Loss: 0.1129\n",
            "Epoch [1/5], Loss: 0.1131\n",
            "Epoch [1/5], Loss: 0.1131\n",
            "Epoch [1/5], Loss: 0.1133\n",
            "Epoch [1/5], Loss: 0.1135\n",
            "Epoch [1/5], Loss: 0.1137\n",
            "Epoch [1/5], Loss: 0.1138\n",
            "Epoch [1/5], Loss: 0.1139\n",
            "Epoch [1/5], Loss: 0.1140\n",
            "Epoch [1/5], Loss: 0.1143\n",
            "Epoch [1/5], Loss: 0.1145\n",
            "Epoch [1/5], Loss: 0.1148\n",
            "Epoch [1/5], Loss: 0.1150\n",
            "Epoch [1/5], Loss: 0.1151\n",
            "Epoch [1/5], Loss: 0.1153\n",
            "Epoch [1/5], Loss: 0.1154\n",
            "Epoch [1/5], Loss: 0.1156\n",
            "Epoch [1/5], Loss: 0.1157\n",
            "Epoch [1/5], Loss: 0.1158\n",
            "Epoch [1/5], Loss: 0.1161\n",
            "Epoch [1/5], Loss: 0.1162\n",
            "Epoch [1/5], Loss: 0.1164\n",
            "Epoch [1/5], Loss: 0.1165\n",
            "Epoch [1/5], Loss: 0.1168\n",
            "Epoch [1/5], Loss: 0.1170\n",
            "Epoch [1/5], Loss: 0.1172\n",
            "Epoch [1/5], Loss: 0.1173\n",
            "Epoch [1/5], Loss: 0.1175\n",
            "Epoch [1/5], Loss: 0.1176\n",
            "Epoch [1/5], Loss: 0.1177\n",
            "Epoch [1/5], Loss: 0.1178\n",
            "Epoch [1/5], Loss: 0.1179\n",
            "Epoch [1/5], Loss: 0.1180\n",
            "Epoch [1/5], Loss: 0.1181\n",
            "Epoch [1/5], Loss: 0.1184\n",
            "Epoch [1/5], Loss: 0.1185\n",
            "Epoch [1/5], Loss: 0.1186\n",
            "Epoch [1/5], Loss: 0.1188\n",
            "Epoch [1/5], Loss: 0.1189\n",
            "Epoch [1/5], Loss: 0.1191\n",
            "Epoch [1/5], Loss: 0.1192\n",
            "Epoch [1/5], Loss: 0.1193\n",
            "Epoch [1/5], Loss: 0.1194\n",
            "Epoch [1/5], Loss: 0.1195\n",
            "Epoch [1/5], Loss: 0.1199\n",
            "Epoch [1/5], Loss: 0.1201\n",
            "Epoch [1/5], Loss: 0.1203\n",
            "Epoch [1/5], Loss: 0.1203\n",
            "Epoch [1/5], Loss: 0.1204\n",
            "Epoch [1/5], Loss: 0.1207\n",
            "Epoch [1/5], Loss: 0.1208\n",
            "Epoch [1/5], Loss: 0.1210\n",
            "Epoch [1/5], Loss: 0.1211\n",
            "Epoch [1/5], Loss: 0.1213\n",
            "Epoch [1/5], Loss: 0.1214\n",
            "Epoch [1/5], Loss: 0.1218\n",
            "Epoch [1/5], Loss: 0.1219\n",
            "Epoch [1/5], Loss: 0.1220\n",
            "Epoch [1/5], Loss: 0.1221\n",
            "Epoch [1/5], Loss: 0.1224\n",
            "Epoch [1/5], Loss: 0.1225\n",
            "Epoch [1/5], Loss: 0.1225\n",
            "Epoch [1/5], Loss: 0.1226\n",
            "Epoch [1/5], Loss: 0.1228\n",
            "Epoch [1/5], Loss: 0.1229\n",
            "Epoch [1/5], Loss: 0.1231\n",
            "Epoch [1/5], Loss: 0.1233\n",
            "Epoch [1/5], Loss: 0.1235\n",
            "Epoch [1/5], Loss: 0.1237\n",
            "Epoch [1/5], Loss: 0.1239\n",
            "Epoch [1/5], Loss: 0.1240\n",
            "Epoch [1/5], Loss: 0.1242\n",
            "Epoch [1/5], Loss: 0.1243\n",
            "Epoch [1/5], Loss: 0.1246\n",
            "Epoch [1/5], Loss: 0.1250\n",
            "Epoch [1/5], Loss: 0.1251\n",
            "Epoch [1/5], Loss: 0.1252\n",
            "Epoch [1/5], Loss: 0.1253\n",
            "Epoch [1/5], Loss: 0.1254\n",
            "Epoch [1/5], Loss: 0.1255\n",
            "Epoch [1/5], Loss: 0.1256\n",
            "Epoch [1/5], Loss: 0.1257\n",
            "Epoch [1/5], Loss: 0.1258\n",
            "Epoch [1/5], Loss: 0.1259\n",
            "Epoch [1/5], Loss: 0.1261\n",
            "Epoch [1/5], Loss: 0.1262\n",
            "Epoch [1/5], Loss: 0.1264\n",
            "Epoch [1/5], Loss: 0.1264\n",
            "Epoch [1/5], Loss: 0.1266\n",
            "Epoch [1/5], Loss: 0.1268\n",
            "Epoch [1/5], Loss: 0.1269\n",
            "Epoch [1/5], Loss: 0.1270\n",
            "Epoch [1/5], Loss: 0.1272\n",
            "Epoch [1/5], Loss: 0.1273\n",
            "Epoch [1/5], Loss: 0.1274\n",
            "Epoch [1/5], Loss: 0.1276\n",
            "Epoch [1/5], Loss: 0.1278\n",
            "Epoch [1/5], Loss: 0.1279\n",
            "Epoch [1/5], Loss: 0.1280\n",
            "Epoch [1/5], Loss: 0.1281\n",
            "Epoch [1/5], Loss: 0.1283\n",
            "Epoch [1/5], Loss: 0.1284\n",
            "Epoch [1/5], Loss: 0.1285\n",
            "Epoch [1/5], Loss: 0.1288\n",
            "Epoch [1/5], Loss: 0.1289\n",
            "Epoch [1/5], Loss: 0.1291\n",
            "Epoch [1/5], Loss: 0.1292\n",
            "Epoch [1/5], Loss: 0.1294\n",
            "Epoch [1/5], Loss: 0.1295\n",
            "Epoch [1/5], Loss: 0.1297\n",
            "Epoch [1/5], Loss: 0.1299\n",
            "Epoch [1/5], Loss: 0.1300\n",
            "Epoch [1/5], Loss: 0.1302\n",
            "Epoch [1/5], Loss: 0.1303\n",
            "Epoch [1/5], Loss: 0.1304\n",
            "Epoch [1/5], Loss: 0.1306\n",
            "Epoch [1/5], Loss: 0.1307\n",
            "Epoch [1/5], Loss: 0.1308\n",
            "Epoch [1/5], Loss: 0.1311\n",
            "Epoch [1/5], Loss: 0.1312\n",
            "Epoch [1/5], Loss: 0.1315\n",
            "Epoch [1/5], Loss: 0.1317\n",
            "Epoch [1/5], Loss: 0.1319\n",
            "Epoch [1/5], Loss: 0.1320\n",
            "Epoch [1/5], Loss: 0.1320\n",
            "Epoch [1/5], Loss: 0.1322\n",
            "Epoch [1/5], Loss: 0.1323\n",
            "Epoch [1/5], Loss: 0.1325\n",
            "Epoch [1/5], Loss: 0.1327\n",
            "Epoch [1/5], Loss: 0.1328\n",
            "Epoch [1/5], Loss: 0.1329\n",
            "Epoch [1/5], Loss: 0.1331\n",
            "Epoch [1/5], Loss: 0.1332\n",
            "Epoch [1/5], Loss: 0.1333\n",
            "Epoch [1/5], Loss: 0.1335\n",
            "Epoch [1/5], Loss: 0.1336\n",
            "Epoch [1/5], Loss: 0.1338\n",
            "Epoch [1/5], Loss: 0.1340\n",
            "Epoch [1/5], Loss: 0.1341\n",
            "Epoch [1/5], Loss: 0.1341\n",
            "Epoch [1/5], Loss: 0.1342\n",
            "Epoch [1/5], Loss: 0.1344\n",
            "Epoch [1/5], Loss: 0.1346\n",
            "Epoch [1/5], Loss: 0.1347\n",
            "Epoch [1/5], Loss: 0.1348\n",
            "Epoch [1/5], Loss: 0.1350\n",
            "Epoch [1/5], Loss: 0.1350\n",
            "Epoch [1/5], Loss: 0.1351\n",
            "Epoch [1/5], Loss: 0.1353\n",
            "Epoch [1/5], Loss: 0.1356\n",
            "Epoch [1/5], Loss: 0.1357\n",
            "Epoch [1/5], Loss: 0.1359\n",
            "Epoch [1/5], Loss: 0.1361\n",
            "Epoch [1/5], Loss: 0.1362\n",
            "Epoch [1/5], Loss: 0.1363\n",
            "Epoch [1/5], Loss: 0.1365\n",
            "Epoch [1/5], Loss: 0.1365\n",
            "Epoch [1/5], Loss: 0.1366\n",
            "Epoch [1/5], Loss: 0.1368\n",
            "Epoch [1/5], Loss: 0.1368\n",
            "Epoch [1/5], Loss: 0.1369\n",
            "Epoch [1/5], Loss: 0.1370\n",
            "Epoch [1/5], Loss: 0.1371\n",
            "Epoch [1/5], Loss: 0.1373\n",
            "Epoch [1/5], Loss: 0.1373\n",
            "Epoch [1/5], Loss: 0.1376\n",
            "Epoch [1/5], Loss: 0.1378\n",
            "Epoch [1/5], Loss: 0.1380\n",
            "Epoch [1/5], Loss: 0.1380\n",
            "Epoch [1/5], Loss: 0.1381\n",
            "Epoch [1/5], Loss: 0.1384\n",
            "Epoch [1/5], Loss: 0.1386\n",
            "Epoch [1/5], Loss: 0.1387\n",
            "Epoch [1/5], Loss: 0.1387\n",
            "Epoch [1/5], Loss: 0.1388\n",
            "Epoch [1/5], Loss: 0.1389\n",
            "Epoch [1/5], Loss: 0.1390\n",
            "Epoch [1/5], Loss: 0.1391\n",
            "Epoch [1/5], Loss: 0.1392\n",
            "Epoch [1/5], Loss: 0.1393\n",
            "Epoch [1/5], Loss: 0.1393\n",
            "Epoch [1/5], Loss: 0.1394\n",
            "Epoch [1/5], Loss: 0.1395\n",
            "Epoch [1/5], Loss: 0.1397\n",
            "Epoch [1/5], Loss: 0.1397\n",
            "Epoch [1/5], Loss: 0.1400\n",
            "Epoch [1/5], Loss: 0.1400\n",
            "Epoch [1/5], Loss: 0.1401\n",
            "Epoch [1/5], Loss: 0.1401\n",
            "Epoch [1/5], Loss: 0.1402\n",
            "Epoch [1/5], Loss: 0.1405\n",
            "Epoch [1/5], Loss: 0.1407\n",
            "Epoch [1/5], Loss: 0.1408\n",
            "Epoch [1/5], Loss: 0.1409\n",
            "Epoch [1/5], Loss: 0.1413\n",
            "Epoch [1/5], Loss: 0.1414\n",
            "Epoch [1/5], Loss: 0.1416\n",
            "Epoch [1/5], Loss: 0.1418\n",
            "Epoch [1/5], Loss: 0.1419\n",
            "Epoch [1/5], Loss: 0.1420\n",
            "Epoch [1/5], Loss: 0.1421\n",
            "Epoch [1/5], Loss: 0.1422\n",
            "Epoch [1/5], Loss: 0.1422\n",
            "Epoch [1/5], Loss: 0.1424\n",
            "Epoch [1/5], Loss: 0.1425\n",
            "Epoch [1/5], Loss: 0.1425\n",
            "Epoch [1/5], Loss: 0.1427\n",
            "Epoch [1/5], Loss: 0.1429\n",
            "Epoch [1/5], Loss: 0.1431\n",
            "Epoch [1/5], Loss: 0.1431\n",
            "Epoch [1/5], Loss: 0.1432\n",
            "Epoch [1/5], Loss: 0.1433\n",
            "Epoch [1/5], Loss: 0.1434\n",
            "Epoch [1/5], Loss: 0.1435\n",
            "Epoch [1/5], Loss: 0.1437\n",
            "Epoch [1/5], Loss: 0.1437\n",
            "Epoch [1/5], Loss: 0.1438\n",
            "Epoch [1/5], Loss: 0.1439\n",
            "Epoch [1/5], Loss: 0.1439\n",
            "Epoch [1/5], Loss: 0.1440\n",
            "Epoch [1/5], Loss: 0.1440\n",
            "Epoch [1/5], Loss: 0.1441\n",
            "Epoch [1/5], Loss: 0.1442\n",
            "Epoch [1/5], Loss: 0.1443\n",
            "Epoch [1/5], Loss: 0.1444\n",
            "Epoch [1/5], Loss: 0.1445\n",
            "Epoch [1/5], Loss: 0.1446\n",
            "Epoch [1/5], Loss: 0.1447\n",
            "Epoch [1/5], Loss: 0.1448\n",
            "Epoch [1/5], Loss: 0.1449\n",
            "Epoch [1/5], Loss: 0.1450\n",
            "Epoch [1/5], Loss: 0.1453\n",
            "Epoch [1/5], Loss: 0.1453\n",
            "Epoch [1/5], Loss: 0.1454\n",
            "Epoch [1/5], Loss: 0.1455\n",
            "Epoch [2/5], Loss: 0.0000\n",
            "Epoch [2/5], Loss: 0.0001\n",
            "Epoch [2/5], Loss: 0.0002\n",
            "Epoch [2/5], Loss: 0.0002\n",
            "Epoch [2/5], Loss: 0.0004\n",
            "Epoch [2/5], Loss: 0.0005\n",
            "Epoch [2/5], Loss: 0.0005\n",
            "Epoch [2/5], Loss: 0.0006\n",
            "Epoch [2/5], Loss: 0.0007\n",
            "Epoch [2/5], Loss: 0.0007\n",
            "Epoch [2/5], Loss: 0.0008\n",
            "Epoch [2/5], Loss: 0.0009\n",
            "Epoch [2/5], Loss: 0.0010\n",
            "Epoch [2/5], Loss: 0.0010\n",
            "Epoch [2/5], Loss: 0.0010\n",
            "Epoch [2/5], Loss: 0.0011\n",
            "Epoch [2/5], Loss: 0.0011\n",
            "Epoch [2/5], Loss: 0.0013\n",
            "Epoch [2/5], Loss: 0.0014\n",
            "Epoch [2/5], Loss: 0.0015\n",
            "Epoch [2/5], Loss: 0.0016\n",
            "Epoch [2/5], Loss: 0.0016\n",
            "Epoch [2/5], Loss: 0.0017\n",
            "Epoch [2/5], Loss: 0.0017\n",
            "Epoch [2/5], Loss: 0.0018\n",
            "Epoch [2/5], Loss: 0.0019\n",
            "Epoch [2/5], Loss: 0.0019\n",
            "Epoch [2/5], Loss: 0.0020\n",
            "Epoch [2/5], Loss: 0.0021\n",
            "Epoch [2/5], Loss: 0.0023\n",
            "Epoch [2/5], Loss: 0.0026\n",
            "Epoch [2/5], Loss: 0.0027\n",
            "Epoch [2/5], Loss: 0.0027\n",
            "Epoch [2/5], Loss: 0.0028\n",
            "Epoch [2/5], Loss: 0.0028\n",
            "Epoch [2/5], Loss: 0.0029\n",
            "Epoch [2/5], Loss: 0.0030\n",
            "Epoch [2/5], Loss: 0.0031\n",
            "Epoch [2/5], Loss: 0.0032\n",
            "Epoch [2/5], Loss: 0.0033\n",
            "Epoch [2/5], Loss: 0.0034\n",
            "Epoch [2/5], Loss: 0.0036\n",
            "Epoch [2/5], Loss: 0.0036\n",
            "Epoch [2/5], Loss: 0.0038\n",
            "Epoch [2/5], Loss: 0.0038\n",
            "Epoch [2/5], Loss: 0.0039\n",
            "Epoch [2/5], Loss: 0.0040\n",
            "Epoch [2/5], Loss: 0.0040\n",
            "Epoch [2/5], Loss: 0.0041\n",
            "Epoch [2/5], Loss: 0.0041\n",
            "Epoch [2/5], Loss: 0.0044\n",
            "Epoch [2/5], Loss: 0.0046\n",
            "Epoch [2/5], Loss: 0.0048\n",
            "Epoch [2/5], Loss: 0.0048\n",
            "Epoch [2/5], Loss: 0.0052\n",
            "Epoch [2/5], Loss: 0.0052\n",
            "Epoch [2/5], Loss: 0.0053\n",
            "Epoch [2/5], Loss: 0.0053\n",
            "Epoch [2/5], Loss: 0.0054\n",
            "Epoch [2/5], Loss: 0.0055\n",
            "Epoch [2/5], Loss: 0.0056\n",
            "Epoch [2/5], Loss: 0.0057\n",
            "Epoch [2/5], Loss: 0.0057\n",
            "Epoch [2/5], Loss: 0.0058\n",
            "Epoch [2/5], Loss: 0.0058\n",
            "Epoch [2/5], Loss: 0.0059\n",
            "Epoch [2/5], Loss: 0.0060\n",
            "Epoch [2/5], Loss: 0.0060\n",
            "Epoch [2/5], Loss: 0.0062\n",
            "Epoch [2/5], Loss: 0.0064\n",
            "Epoch [2/5], Loss: 0.0065\n",
            "Epoch [2/5], Loss: 0.0065\n",
            "Epoch [2/5], Loss: 0.0067\n",
            "Epoch [2/5], Loss: 0.0068\n",
            "Epoch [2/5], Loss: 0.0069\n",
            "Epoch [2/5], Loss: 0.0070\n",
            "Epoch [2/5], Loss: 0.0070\n",
            "Epoch [2/5], Loss: 0.0071\n",
            "Epoch [2/5], Loss: 0.0072\n",
            "Epoch [2/5], Loss: 0.0072\n",
            "Epoch [2/5], Loss: 0.0073\n",
            "Epoch [2/5], Loss: 0.0074\n",
            "Epoch [2/5], Loss: 0.0074\n",
            "Epoch [2/5], Loss: 0.0074\n",
            "Epoch [2/5], Loss: 0.0075\n",
            "Epoch [2/5], Loss: 0.0076\n",
            "Epoch [2/5], Loss: 0.0079\n",
            "Epoch [2/5], Loss: 0.0079\n",
            "Epoch [2/5], Loss: 0.0080\n",
            "Epoch [2/5], Loss: 0.0080\n",
            "Epoch [2/5], Loss: 0.0081\n",
            "Epoch [2/5], Loss: 0.0081\n",
            "Epoch [2/5], Loss: 0.0083\n",
            "Epoch [2/5], Loss: 0.0084\n",
            "Epoch [2/5], Loss: 0.0085\n",
            "Epoch [2/5], Loss: 0.0086\n",
            "Epoch [2/5], Loss: 0.0088\n",
            "Epoch [2/5], Loss: 0.0088\n",
            "Epoch [2/5], Loss: 0.0089\n",
            "Epoch [2/5], Loss: 0.0090\n",
            "Epoch [2/5], Loss: 0.0091\n",
            "Epoch [2/5], Loss: 0.0092\n",
            "Epoch [2/5], Loss: 0.0093\n",
            "Epoch [2/5], Loss: 0.0095\n",
            "Epoch [2/5], Loss: 0.0095\n",
            "Epoch [2/5], Loss: 0.0095\n",
            "Epoch [2/5], Loss: 0.0096\n",
            "Epoch [2/5], Loss: 0.0097\n",
            "Epoch [2/5], Loss: 0.0097\n",
            "Epoch [2/5], Loss: 0.0098\n",
            "Epoch [2/5], Loss: 0.0099\n",
            "Epoch [2/5], Loss: 0.0099\n",
            "Epoch [2/5], Loss: 0.0100\n",
            "Epoch [2/5], Loss: 0.0101\n",
            "Epoch [2/5], Loss: 0.0102\n",
            "Epoch [2/5], Loss: 0.0102\n",
            "Epoch [2/5], Loss: 0.0104\n",
            "Epoch [2/5], Loss: 0.0107\n",
            "Epoch [2/5], Loss: 0.0107\n",
            "Epoch [2/5], Loss: 0.0108\n",
            "Epoch [2/5], Loss: 0.0109\n",
            "Epoch [2/5], Loss: 0.0109\n",
            "Epoch [2/5], Loss: 0.0111\n",
            "Epoch [2/5], Loss: 0.0111\n",
            "Epoch [2/5], Loss: 0.0112\n",
            "Epoch [2/5], Loss: 0.0113\n",
            "Epoch [2/5], Loss: 0.0114\n",
            "Epoch [2/5], Loss: 0.0114\n",
            "Epoch [2/5], Loss: 0.0115\n",
            "Epoch [2/5], Loss: 0.0116\n",
            "Epoch [2/5], Loss: 0.0117\n",
            "Epoch [2/5], Loss: 0.0117\n",
            "Epoch [2/5], Loss: 0.0119\n",
            "Epoch [2/5], Loss: 0.0119\n",
            "Epoch [2/5], Loss: 0.0119\n",
            "Epoch [2/5], Loss: 0.0120\n",
            "Epoch [2/5], Loss: 0.0120\n",
            "Epoch [2/5], Loss: 0.0122\n",
            "Epoch [2/5], Loss: 0.0122\n",
            "Epoch [2/5], Loss: 0.0122\n",
            "Epoch [2/5], Loss: 0.0123\n",
            "Epoch [2/5], Loss: 0.0124\n",
            "Epoch [2/5], Loss: 0.0124\n",
            "Epoch [2/5], Loss: 0.0126\n",
            "Epoch [2/5], Loss: 0.0128\n",
            "Epoch [2/5], Loss: 0.0128\n",
            "Epoch [2/5], Loss: 0.0129\n",
            "Epoch [2/5], Loss: 0.0131\n",
            "Epoch [2/5], Loss: 0.0131\n",
            "Epoch [2/5], Loss: 0.0131\n",
            "Epoch [2/5], Loss: 0.0132\n",
            "Epoch [2/5], Loss: 0.0133\n",
            "Epoch [2/5], Loss: 0.0133\n",
            "Epoch [2/5], Loss: 0.0134\n",
            "Epoch [2/5], Loss: 0.0135\n",
            "Epoch [2/5], Loss: 0.0136\n",
            "Epoch [2/5], Loss: 0.0136\n",
            "Epoch [2/5], Loss: 0.0137\n",
            "Epoch [2/5], Loss: 0.0138\n",
            "Epoch [2/5], Loss: 0.0138\n",
            "Epoch [2/5], Loss: 0.0139\n",
            "Epoch [2/5], Loss: 0.0139\n",
            "Epoch [2/5], Loss: 0.0140\n",
            "Epoch [2/5], Loss: 0.0140\n",
            "Epoch [2/5], Loss: 0.0141\n",
            "Epoch [2/5], Loss: 0.0142\n",
            "Epoch [2/5], Loss: 0.0142\n",
            "Epoch [2/5], Loss: 0.0143\n",
            "Epoch [2/5], Loss: 0.0146\n",
            "Epoch [2/5], Loss: 0.0147\n",
            "Epoch [2/5], Loss: 0.0149\n",
            "Epoch [2/5], Loss: 0.0150\n",
            "Epoch [2/5], Loss: 0.0153\n",
            "Epoch [2/5], Loss: 0.0154\n",
            "Epoch [2/5], Loss: 0.0155\n",
            "Epoch [2/5], Loss: 0.0155\n",
            "Epoch [2/5], Loss: 0.0157\n",
            "Epoch [2/5], Loss: 0.0157\n",
            "Epoch [2/5], Loss: 0.0158\n",
            "Epoch [2/5], Loss: 0.0159\n",
            "Epoch [2/5], Loss: 0.0160\n",
            "Epoch [2/5], Loss: 0.0161\n",
            "Epoch [2/5], Loss: 0.0162\n",
            "Epoch [2/5], Loss: 0.0162\n",
            "Epoch [2/5], Loss: 0.0164\n",
            "Epoch [2/5], Loss: 0.0167\n",
            "Epoch [2/5], Loss: 0.0169\n",
            "Epoch [2/5], Loss: 0.0169\n",
            "Epoch [2/5], Loss: 0.0170\n",
            "Epoch [2/5], Loss: 0.0172\n",
            "Epoch [2/5], Loss: 0.0174\n",
            "Epoch [2/5], Loss: 0.0176\n",
            "Epoch [2/5], Loss: 0.0178\n",
            "Epoch [2/5], Loss: 0.0179\n",
            "Epoch [2/5], Loss: 0.0181\n",
            "Epoch [2/5], Loss: 0.0182\n",
            "Epoch [2/5], Loss: 0.0183\n",
            "Epoch [2/5], Loss: 0.0184\n",
            "Epoch [2/5], Loss: 0.0184\n",
            "Epoch [2/5], Loss: 0.0185\n",
            "Epoch [2/5], Loss: 0.0186\n",
            "Epoch [2/5], Loss: 0.0186\n",
            "Epoch [2/5], Loss: 0.0187\n",
            "Epoch [2/5], Loss: 0.0189\n",
            "Epoch [2/5], Loss: 0.0190\n",
            "Epoch [2/5], Loss: 0.0191\n",
            "Epoch [2/5], Loss: 0.0191\n",
            "Epoch [2/5], Loss: 0.0193\n",
            "Epoch [2/5], Loss: 0.0195\n",
            "Epoch [2/5], Loss: 0.0195\n",
            "Epoch [2/5], Loss: 0.0196\n",
            "Epoch [2/5], Loss: 0.0196\n",
            "Epoch [2/5], Loss: 0.0196\n",
            "Epoch [2/5], Loss: 0.0196\n",
            "Epoch [2/5], Loss: 0.0198\n",
            "Epoch [2/5], Loss: 0.0198\n",
            "Epoch [2/5], Loss: 0.0199\n",
            "Epoch [2/5], Loss: 0.0200\n",
            "Epoch [2/5], Loss: 0.0201\n",
            "Epoch [2/5], Loss: 0.0203\n",
            "Epoch [2/5], Loss: 0.0204\n",
            "Epoch [2/5], Loss: 0.0204\n",
            "Epoch [2/5], Loss: 0.0205\n",
            "Epoch [2/5], Loss: 0.0206\n",
            "Epoch [2/5], Loss: 0.0207\n",
            "Epoch [2/5], Loss: 0.0208\n",
            "Epoch [2/5], Loss: 0.0208\n",
            "Epoch [2/5], Loss: 0.0210\n",
            "Epoch [2/5], Loss: 0.0211\n",
            "Epoch [2/5], Loss: 0.0212\n",
            "Epoch [2/5], Loss: 0.0212\n",
            "Epoch [2/5], Loss: 0.0214\n",
            "Epoch [2/5], Loss: 0.0214\n",
            "Epoch [2/5], Loss: 0.0215\n",
            "Epoch [2/5], Loss: 0.0216\n",
            "Epoch [2/5], Loss: 0.0218\n",
            "Epoch [2/5], Loss: 0.0218\n",
            "Epoch [2/5], Loss: 0.0219\n",
            "Epoch [2/5], Loss: 0.0219\n",
            "Epoch [2/5], Loss: 0.0221\n",
            "Epoch [2/5], Loss: 0.0222\n",
            "Epoch [2/5], Loss: 0.0223\n",
            "Epoch [2/5], Loss: 0.0224\n",
            "Epoch [2/5], Loss: 0.0224\n",
            "Epoch [2/5], Loss: 0.0225\n",
            "Epoch [2/5], Loss: 0.0225\n",
            "Epoch [2/5], Loss: 0.0225\n",
            "Epoch [2/5], Loss: 0.0228\n",
            "Epoch [2/5], Loss: 0.0230\n",
            "Epoch [2/5], Loss: 0.0230\n",
            "Epoch [2/5], Loss: 0.0231\n",
            "Epoch [2/5], Loss: 0.0231\n",
            "Epoch [2/5], Loss: 0.0232\n",
            "Epoch [2/5], Loss: 0.0232\n",
            "Epoch [2/5], Loss: 0.0234\n",
            "Epoch [2/5], Loss: 0.0235\n",
            "Epoch [2/5], Loss: 0.0235\n",
            "Epoch [2/5], Loss: 0.0236\n",
            "Epoch [2/5], Loss: 0.0237\n",
            "Epoch [2/5], Loss: 0.0237\n",
            "Epoch [2/5], Loss: 0.0238\n",
            "Epoch [2/5], Loss: 0.0239\n",
            "Epoch [2/5], Loss: 0.0239\n",
            "Epoch [2/5], Loss: 0.0240\n",
            "Epoch [2/5], Loss: 0.0240\n",
            "Epoch [2/5], Loss: 0.0240\n",
            "Epoch [2/5], Loss: 0.0240\n",
            "Epoch [2/5], Loss: 0.0241\n",
            "Epoch [2/5], Loss: 0.0241\n",
            "Epoch [2/5], Loss: 0.0243\n",
            "Epoch [2/5], Loss: 0.0243\n",
            "Epoch [2/5], Loss: 0.0245\n",
            "Epoch [2/5], Loss: 0.0245\n",
            "Epoch [2/5], Loss: 0.0247\n",
            "Epoch [2/5], Loss: 0.0249\n",
            "Epoch [2/5], Loss: 0.0249\n",
            "Epoch [2/5], Loss: 0.0249\n",
            "Epoch [2/5], Loss: 0.0250\n",
            "Epoch [2/5], Loss: 0.0251\n",
            "Epoch [2/5], Loss: 0.0252\n",
            "Epoch [2/5], Loss: 0.0253\n",
            "Epoch [2/5], Loss: 0.0253\n",
            "Epoch [2/5], Loss: 0.0254\n",
            "Epoch [2/5], Loss: 0.0256\n",
            "Epoch [2/5], Loss: 0.0256\n",
            "Epoch [2/5], Loss: 0.0257\n",
            "Epoch [2/5], Loss: 0.0257\n",
            "Epoch [2/5], Loss: 0.0258\n",
            "Epoch [2/5], Loss: 0.0259\n",
            "Epoch [2/5], Loss: 0.0259\n",
            "Epoch [2/5], Loss: 0.0261\n",
            "Epoch [2/5], Loss: 0.0262\n",
            "Epoch [2/5], Loss: 0.0262\n",
            "Epoch [2/5], Loss: 0.0263\n",
            "Epoch [2/5], Loss: 0.0263\n",
            "Epoch [2/5], Loss: 0.0264\n",
            "Epoch [2/5], Loss: 0.0265\n",
            "Epoch [2/5], Loss: 0.0265\n",
            "Epoch [2/5], Loss: 0.0268\n",
            "Epoch [2/5], Loss: 0.0268\n",
            "Epoch [2/5], Loss: 0.0270\n",
            "Epoch [2/5], Loss: 0.0271\n",
            "Epoch [2/5], Loss: 0.0272\n",
            "Epoch [2/5], Loss: 0.0272\n",
            "Epoch [2/5], Loss: 0.0274\n",
            "Epoch [2/5], Loss: 0.0274\n",
            "Epoch [2/5], Loss: 0.0275\n",
            "Epoch [2/5], Loss: 0.0275\n",
            "Epoch [2/5], Loss: 0.0275\n",
            "Epoch [2/5], Loss: 0.0275\n",
            "Epoch [2/5], Loss: 0.0276\n",
            "Epoch [2/5], Loss: 0.0277\n",
            "Epoch [2/5], Loss: 0.0280\n",
            "Epoch [2/5], Loss: 0.0280\n",
            "Epoch [2/5], Loss: 0.0280\n",
            "Epoch [2/5], Loss: 0.0281\n",
            "Epoch [2/5], Loss: 0.0281\n",
            "Epoch [2/5], Loss: 0.0282\n",
            "Epoch [2/5], Loss: 0.0282\n",
            "Epoch [2/5], Loss: 0.0284\n",
            "Epoch [2/5], Loss: 0.0286\n",
            "Epoch [2/5], Loss: 0.0287\n",
            "Epoch [2/5], Loss: 0.0288\n",
            "Epoch [2/5], Loss: 0.0288\n",
            "Epoch [2/5], Loss: 0.0289\n",
            "Epoch [2/5], Loss: 0.0289\n",
            "Epoch [2/5], Loss: 0.0290\n",
            "Epoch [2/5], Loss: 0.0290\n",
            "Epoch [2/5], Loss: 0.0291\n",
            "Epoch [2/5], Loss: 0.0292\n",
            "Epoch [2/5], Loss: 0.0293\n",
            "Epoch [2/5], Loss: 0.0294\n",
            "Epoch [2/5], Loss: 0.0296\n",
            "Epoch [2/5], Loss: 0.0297\n",
            "Epoch [2/5], Loss: 0.0298\n",
            "Epoch [2/5], Loss: 0.0300\n",
            "Epoch [2/5], Loss: 0.0301\n",
            "Epoch [2/5], Loss: 0.0302\n",
            "Epoch [2/5], Loss: 0.0302\n",
            "Epoch [2/5], Loss: 0.0303\n",
            "Epoch [2/5], Loss: 0.0304\n",
            "Epoch [2/5], Loss: 0.0304\n",
            "Epoch [2/5], Loss: 0.0305\n",
            "Epoch [2/5], Loss: 0.0306\n",
            "Epoch [2/5], Loss: 0.0306\n",
            "Epoch [2/5], Loss: 0.0307\n",
            "Epoch [2/5], Loss: 0.0308\n",
            "Epoch [2/5], Loss: 0.0308\n",
            "Epoch [2/5], Loss: 0.0309\n",
            "Epoch [2/5], Loss: 0.0310\n",
            "Epoch [2/5], Loss: 0.0310\n",
            "Epoch [2/5], Loss: 0.0312\n",
            "Epoch [2/5], Loss: 0.0314\n",
            "Epoch [2/5], Loss: 0.0314\n",
            "Epoch [2/5], Loss: 0.0315\n",
            "Epoch [2/5], Loss: 0.0315\n",
            "Epoch [2/5], Loss: 0.0316\n",
            "Epoch [2/5], Loss: 0.0316\n",
            "Epoch [2/5], Loss: 0.0317\n",
            "Epoch [2/5], Loss: 0.0318\n",
            "Epoch [2/5], Loss: 0.0318\n",
            "Epoch [2/5], Loss: 0.0319\n",
            "Epoch [2/5], Loss: 0.0320\n",
            "Epoch [2/5], Loss: 0.0321\n",
            "Epoch [2/5], Loss: 0.0322\n",
            "Epoch [2/5], Loss: 0.0323\n",
            "Epoch [2/5], Loss: 0.0324\n",
            "Epoch [2/5], Loss: 0.0325\n",
            "Epoch [2/5], Loss: 0.0326\n",
            "Epoch [2/5], Loss: 0.0326\n",
            "Epoch [2/5], Loss: 0.0326\n",
            "Epoch [2/5], Loss: 0.0326\n",
            "Epoch [2/5], Loss: 0.0327\n",
            "Epoch [2/5], Loss: 0.0327\n",
            "Epoch [2/5], Loss: 0.0328\n",
            "Epoch [2/5], Loss: 0.0328\n",
            "Epoch [2/5], Loss: 0.0329\n",
            "Epoch [2/5], Loss: 0.0330\n",
            "Epoch [2/5], Loss: 0.0332\n",
            "Epoch [2/5], Loss: 0.0333\n",
            "Epoch [2/5], Loss: 0.0334\n",
            "Epoch [2/5], Loss: 0.0334\n",
            "Epoch [2/5], Loss: 0.0335\n",
            "Epoch [2/5], Loss: 0.0337\n",
            "Epoch [2/5], Loss: 0.0338\n",
            "Epoch [2/5], Loss: 0.0338\n",
            "Epoch [2/5], Loss: 0.0338\n",
            "Epoch [2/5], Loss: 0.0339\n",
            "Epoch [2/5], Loss: 0.0340\n",
            "Epoch [2/5], Loss: 0.0340\n",
            "Epoch [2/5], Loss: 0.0341\n",
            "Epoch [2/5], Loss: 0.0343\n",
            "Epoch [2/5], Loss: 0.0343\n",
            "Epoch [2/5], Loss: 0.0344\n",
            "Epoch [2/5], Loss: 0.0345\n",
            "Epoch [2/5], Loss: 0.0345\n",
            "Epoch [2/5], Loss: 0.0346\n",
            "Epoch [2/5], Loss: 0.0347\n",
            "Epoch [2/5], Loss: 0.0348\n",
            "Epoch [2/5], Loss: 0.0348\n",
            "Epoch [2/5], Loss: 0.0350\n",
            "Epoch [2/5], Loss: 0.0350\n",
            "Epoch [2/5], Loss: 0.0352\n",
            "Epoch [2/5], Loss: 0.0353\n",
            "Epoch [2/5], Loss: 0.0354\n",
            "Epoch [2/5], Loss: 0.0355\n",
            "Epoch [2/5], Loss: 0.0355\n",
            "Epoch [2/5], Loss: 0.0356\n",
            "Epoch [2/5], Loss: 0.0356\n",
            "Epoch [2/5], Loss: 0.0357\n",
            "Epoch [2/5], Loss: 0.0359\n",
            "Epoch [2/5], Loss: 0.0361\n",
            "Epoch [2/5], Loss: 0.0361\n",
            "Epoch [2/5], Loss: 0.0362\n",
            "Epoch [2/5], Loss: 0.0363\n",
            "Epoch [2/5], Loss: 0.0363\n",
            "Epoch [2/5], Loss: 0.0365\n",
            "Epoch [2/5], Loss: 0.0365\n",
            "Epoch [2/5], Loss: 0.0366\n",
            "Epoch [2/5], Loss: 0.0366\n",
            "Epoch [2/5], Loss: 0.0367\n",
            "Epoch [2/5], Loss: 0.0367\n",
            "Epoch [2/5], Loss: 0.0368\n",
            "Epoch [2/5], Loss: 0.0368\n",
            "Epoch [2/5], Loss: 0.0368\n",
            "Epoch [2/5], Loss: 0.0369\n",
            "Epoch [2/5], Loss: 0.0370\n",
            "Epoch [2/5], Loss: 0.0371\n",
            "Epoch [2/5], Loss: 0.0372\n",
            "Epoch [2/5], Loss: 0.0373\n",
            "Epoch [2/5], Loss: 0.0374\n",
            "Epoch [2/5], Loss: 0.0375\n",
            "Epoch [2/5], Loss: 0.0377\n",
            "Epoch [2/5], Loss: 0.0378\n",
            "Epoch [2/5], Loss: 0.0378\n",
            "Epoch [2/5], Loss: 0.0380\n",
            "Epoch [2/5], Loss: 0.0381\n",
            "Epoch [2/5], Loss: 0.0381\n",
            "Epoch [2/5], Loss: 0.0382\n",
            "Epoch [2/5], Loss: 0.0382\n",
            "Epoch [2/5], Loss: 0.0384\n",
            "Epoch [2/5], Loss: 0.0385\n",
            "Epoch [2/5], Loss: 0.0386\n",
            "Epoch [2/5], Loss: 0.0387\n",
            "Epoch [2/5], Loss: 0.0388\n",
            "Epoch [2/5], Loss: 0.0389\n",
            "Epoch [2/5], Loss: 0.0390\n",
            "Epoch [2/5], Loss: 0.0390\n",
            "Epoch [2/5], Loss: 0.0391\n",
            "Epoch [2/5], Loss: 0.0392\n",
            "Epoch [2/5], Loss: 0.0393\n",
            "Epoch [2/5], Loss: 0.0395\n",
            "Epoch [2/5], Loss: 0.0395\n",
            "Epoch [2/5], Loss: 0.0396\n",
            "Epoch [2/5], Loss: 0.0397\n",
            "Epoch [2/5], Loss: 0.0398\n",
            "Epoch [2/5], Loss: 0.0398\n",
            "Epoch [2/5], Loss: 0.0399\n",
            "Epoch [2/5], Loss: 0.0399\n",
            "Epoch [2/5], Loss: 0.0400\n",
            "Epoch [2/5], Loss: 0.0401\n",
            "Epoch [2/5], Loss: 0.0403\n",
            "Epoch [2/5], Loss: 0.0403\n",
            "Epoch [2/5], Loss: 0.0404\n",
            "Epoch [2/5], Loss: 0.0404\n",
            "Epoch [2/5], Loss: 0.0404\n",
            "Epoch [2/5], Loss: 0.0405\n",
            "Epoch [2/5], Loss: 0.0406\n",
            "Epoch [2/5], Loss: 0.0407\n",
            "Epoch [3/5], Loss: 0.0001\n",
            "Epoch [3/5], Loss: 0.0003\n",
            "Epoch [3/5], Loss: 0.0003\n",
            "Epoch [3/5], Loss: 0.0004\n",
            "Epoch [3/5], Loss: 0.0004\n",
            "Epoch [3/5], Loss: 0.0004\n",
            "Epoch [3/5], Loss: 0.0004\n",
            "Epoch [3/5], Loss: 0.0004\n",
            "Epoch [3/5], Loss: 0.0005\n",
            "Epoch [3/5], Loss: 0.0005\n",
            "Epoch [3/5], Loss: 0.0006\n",
            "Epoch [3/5], Loss: 0.0006\n",
            "Epoch [3/5], Loss: 0.0007\n",
            "Epoch [3/5], Loss: 0.0007\n",
            "Epoch [3/5], Loss: 0.0009\n",
            "Epoch [3/5], Loss: 0.0009\n",
            "Epoch [3/5], Loss: 0.0009\n",
            "Epoch [3/5], Loss: 0.0010\n",
            "Epoch [3/5], Loss: 0.0010\n",
            "Epoch [3/5], Loss: 0.0012\n",
            "Epoch [3/5], Loss: 0.0012\n",
            "Epoch [3/5], Loss: 0.0012\n",
            "Epoch [3/5], Loss: 0.0012\n",
            "Epoch [3/5], Loss: 0.0015\n",
            "Epoch [3/5], Loss: 0.0015\n",
            "Epoch [3/5], Loss: 0.0016\n",
            "Epoch [3/5], Loss: 0.0017\n",
            "Epoch [3/5], Loss: 0.0017\n",
            "Epoch [3/5], Loss: 0.0017\n",
            "Epoch [3/5], Loss: 0.0018\n",
            "Epoch [3/5], Loss: 0.0018\n",
            "Epoch [3/5], Loss: 0.0019\n",
            "Epoch [3/5], Loss: 0.0020\n",
            "Epoch [3/5], Loss: 0.0021\n",
            "Epoch [3/5], Loss: 0.0021\n",
            "Epoch [3/5], Loss: 0.0022\n",
            "Epoch [3/5], Loss: 0.0023\n",
            "Epoch [3/5], Loss: 0.0023\n",
            "Epoch [3/5], Loss: 0.0024\n",
            "Epoch [3/5], Loss: 0.0024\n",
            "Epoch [3/5], Loss: 0.0025\n",
            "Epoch [3/5], Loss: 0.0026\n",
            "Epoch [3/5], Loss: 0.0027\n",
            "Epoch [3/5], Loss: 0.0027\n",
            "Epoch [3/5], Loss: 0.0027\n",
            "Epoch [3/5], Loss: 0.0028\n",
            "Epoch [3/5], Loss: 0.0028\n",
            "Epoch [3/5], Loss: 0.0028\n",
            "Epoch [3/5], Loss: 0.0029\n",
            "Epoch [3/5], Loss: 0.0029\n",
            "Epoch [3/5], Loss: 0.0030\n",
            "Epoch [3/5], Loss: 0.0030\n",
            "Epoch [3/5], Loss: 0.0030\n",
            "Epoch [3/5], Loss: 0.0030\n",
            "Epoch [3/5], Loss: 0.0031\n",
            "Epoch [3/5], Loss: 0.0031\n",
            "Epoch [3/5], Loss: 0.0032\n",
            "Epoch [3/5], Loss: 0.0033\n",
            "Epoch [3/5], Loss: 0.0033\n",
            "Epoch [3/5], Loss: 0.0034\n",
            "Epoch [3/5], Loss: 0.0034\n",
            "Epoch [3/5], Loss: 0.0035\n",
            "Epoch [3/5], Loss: 0.0035\n",
            "Epoch [3/5], Loss: 0.0035\n",
            "Epoch [3/5], Loss: 0.0036\n",
            "Epoch [3/5], Loss: 0.0036\n",
            "Epoch [3/5], Loss: 0.0036\n",
            "Epoch [3/5], Loss: 0.0037\n",
            "Epoch [3/5], Loss: 0.0038\n",
            "Epoch [3/5], Loss: 0.0038\n",
            "Epoch [3/5], Loss: 0.0039\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0040\n",
            "Epoch [3/5], Loss: 0.0041\n",
            "Epoch [3/5], Loss: 0.0041\n",
            "Epoch [3/5], Loss: 0.0041\n",
            "Epoch [3/5], Loss: 0.0042\n",
            "Epoch [3/5], Loss: 0.0042\n",
            "Epoch [3/5], Loss: 0.0042\n",
            "Epoch [3/5], Loss: 0.0043\n",
            "Epoch [3/5], Loss: 0.0044\n",
            "Epoch [3/5], Loss: 0.0044\n",
            "Epoch [3/5], Loss: 0.0045\n",
            "Epoch [3/5], Loss: 0.0045\n",
            "Epoch [3/5], Loss: 0.0046\n",
            "Epoch [3/5], Loss: 0.0046\n",
            "Epoch [3/5], Loss: 0.0046\n",
            "Epoch [3/5], Loss: 0.0046\n",
            "Epoch [3/5], Loss: 0.0047\n",
            "Epoch [3/5], Loss: 0.0047\n",
            "Epoch [3/5], Loss: 0.0048\n",
            "Epoch [3/5], Loss: 0.0048\n",
            "Epoch [3/5], Loss: 0.0049\n",
            "Epoch [3/5], Loss: 0.0050\n",
            "Epoch [3/5], Loss: 0.0050\n",
            "Epoch [3/5], Loss: 0.0051\n",
            "Epoch [3/5], Loss: 0.0052\n",
            "Epoch [3/5], Loss: 0.0052\n",
            "Epoch [3/5], Loss: 0.0052\n",
            "Epoch [3/5], Loss: 0.0053\n",
            "Epoch [3/5], Loss: 0.0053\n",
            "Epoch [3/5], Loss: 0.0053\n",
            "Epoch [3/5], Loss: 0.0054\n",
            "Epoch [3/5], Loss: 0.0054\n",
            "Epoch [3/5], Loss: 0.0055\n",
            "Epoch [3/5], Loss: 0.0056\n",
            "Epoch [3/5], Loss: 0.0056\n",
            "Epoch [3/5], Loss: 0.0057\n",
            "Epoch [3/5], Loss: 0.0058\n",
            "Epoch [3/5], Loss: 0.0058\n",
            "Epoch [3/5], Loss: 0.0058\n",
            "Epoch [3/5], Loss: 0.0059\n",
            "Epoch [3/5], Loss: 0.0059\n",
            "Epoch [3/5], Loss: 0.0062\n",
            "Epoch [3/5], Loss: 0.0062\n",
            "Epoch [3/5], Loss: 0.0063\n",
            "Epoch [3/5], Loss: 0.0063\n",
            "Epoch [3/5], Loss: 0.0063\n",
            "Epoch [3/5], Loss: 0.0064\n",
            "Epoch [3/5], Loss: 0.0064\n",
            "Epoch [3/5], Loss: 0.0065\n",
            "Epoch [3/5], Loss: 0.0066\n",
            "Epoch [3/5], Loss: 0.0067\n",
            "Epoch [3/5], Loss: 0.0068\n",
            "Epoch [3/5], Loss: 0.0068\n",
            "Epoch [3/5], Loss: 0.0068\n",
            "Epoch [3/5], Loss: 0.0069\n",
            "Epoch [3/5], Loss: 0.0069\n",
            "Epoch [3/5], Loss: 0.0069\n",
            "Epoch [3/5], Loss: 0.0069\n",
            "Epoch [3/5], Loss: 0.0070\n",
            "Epoch [3/5], Loss: 0.0070\n",
            "Epoch [3/5], Loss: 0.0071\n",
            "Epoch [3/5], Loss: 0.0072\n",
            "Epoch [3/5], Loss: 0.0073\n",
            "Epoch [3/5], Loss: 0.0074\n",
            "Epoch [3/5], Loss: 0.0074\n",
            "Epoch [3/5], Loss: 0.0074\n",
            "Epoch [3/5], Loss: 0.0075\n",
            "Epoch [3/5], Loss: 0.0075\n",
            "Epoch [3/5], Loss: 0.0075\n",
            "Epoch [3/5], Loss: 0.0077\n",
            "Epoch [3/5], Loss: 0.0077\n",
            "Epoch [3/5], Loss: 0.0077\n",
            "Epoch [3/5], Loss: 0.0078\n",
            "Epoch [3/5], Loss: 0.0078\n",
            "Epoch [3/5], Loss: 0.0079\n",
            "Epoch [3/5], Loss: 0.0079\n",
            "Epoch [3/5], Loss: 0.0080\n",
            "Epoch [3/5], Loss: 0.0080\n",
            "Epoch [3/5], Loss: 0.0081\n",
            "Epoch [3/5], Loss: 0.0081\n",
            "Epoch [3/5], Loss: 0.0081\n",
            "Epoch [3/5], Loss: 0.0082\n",
            "Epoch [3/5], Loss: 0.0082\n",
            "Epoch [3/5], Loss: 0.0082\n",
            "Epoch [3/5], Loss: 0.0083\n",
            "Epoch [3/5], Loss: 0.0083\n",
            "Epoch [3/5], Loss: 0.0083\n",
            "Epoch [3/5], Loss: 0.0084\n",
            "Epoch [3/5], Loss: 0.0084\n",
            "Epoch [3/5], Loss: 0.0085\n",
            "Epoch [3/5], Loss: 0.0086\n",
            "Epoch [3/5], Loss: 0.0086\n",
            "Epoch [3/5], Loss: 0.0086\n",
            "Epoch [3/5], Loss: 0.0087\n",
            "Epoch [3/5], Loss: 0.0089\n",
            "Epoch [3/5], Loss: 0.0090\n",
            "Epoch [3/5], Loss: 0.0091\n",
            "Epoch [3/5], Loss: 0.0091\n",
            "Epoch [3/5], Loss: 0.0091\n",
            "Epoch [3/5], Loss: 0.0091\n",
            "Epoch [3/5], Loss: 0.0092\n",
            "Epoch [3/5], Loss: 0.0092\n",
            "Epoch [3/5], Loss: 0.0093\n",
            "Epoch [3/5], Loss: 0.0094\n",
            "Epoch [3/5], Loss: 0.0094\n",
            "Epoch [3/5], Loss: 0.0094\n",
            "Epoch [3/5], Loss: 0.0095\n",
            "Epoch [3/5], Loss: 0.0096\n",
            "Epoch [3/5], Loss: 0.0096\n",
            "Epoch [3/5], Loss: 0.0096\n",
            "Epoch [3/5], Loss: 0.0096\n",
            "Epoch [3/5], Loss: 0.0097\n",
            "Epoch [3/5], Loss: 0.0098\n",
            "Epoch [3/5], Loss: 0.0099\n",
            "Epoch [3/5], Loss: 0.0099\n",
            "Epoch [3/5], Loss: 0.0099\n",
            "Epoch [3/5], Loss: 0.0101\n",
            "Epoch [3/5], Loss: 0.0101\n",
            "Epoch [3/5], Loss: 0.0102\n",
            "Epoch [3/5], Loss: 0.0102\n",
            "Epoch [3/5], Loss: 0.0103\n",
            "Epoch [3/5], Loss: 0.0103\n",
            "Epoch [3/5], Loss: 0.0104\n",
            "Epoch [3/5], Loss: 0.0104\n",
            "Epoch [3/5], Loss: 0.0105\n",
            "Epoch [3/5], Loss: 0.0105\n",
            "Epoch [3/5], Loss: 0.0106\n",
            "Epoch [3/5], Loss: 0.0106\n",
            "Epoch [3/5], Loss: 0.0107\n",
            "Epoch [3/5], Loss: 0.0108\n",
            "Epoch [3/5], Loss: 0.0108\n",
            "Epoch [3/5], Loss: 0.0108\n",
            "Epoch [3/5], Loss: 0.0110\n",
            "Epoch [3/5], Loss: 0.0110\n",
            "Epoch [3/5], Loss: 0.0110\n",
            "Epoch [3/5], Loss: 0.0110\n",
            "Epoch [3/5], Loss: 0.0111\n",
            "Epoch [3/5], Loss: 0.0111\n",
            "Epoch [3/5], Loss: 0.0112\n",
            "Epoch [3/5], Loss: 0.0112\n",
            "Epoch [3/5], Loss: 0.0113\n",
            "Epoch [3/5], Loss: 0.0113\n",
            "Epoch [3/5], Loss: 0.0113\n",
            "Epoch [3/5], Loss: 0.0113\n",
            "Epoch [3/5], Loss: 0.0114\n",
            "Epoch [3/5], Loss: 0.0114\n",
            "Epoch [3/5], Loss: 0.0115\n",
            "Epoch [3/5], Loss: 0.0115\n",
            "Epoch [3/5], Loss: 0.0116\n",
            "Epoch [3/5], Loss: 0.0116\n",
            "Epoch [3/5], Loss: 0.0117\n",
            "Epoch [3/5], Loss: 0.0118\n",
            "Epoch [3/5], Loss: 0.0118\n",
            "Epoch [3/5], Loss: 0.0118\n",
            "Epoch [3/5], Loss: 0.0119\n",
            "Epoch [3/5], Loss: 0.0120\n",
            "Epoch [3/5], Loss: 0.0121\n",
            "Epoch [3/5], Loss: 0.0121\n",
            "Epoch [3/5], Loss: 0.0122\n",
            "Epoch [3/5], Loss: 0.0122\n",
            "Epoch [3/5], Loss: 0.0122\n",
            "Epoch [3/5], Loss: 0.0122\n",
            "Epoch [3/5], Loss: 0.0123\n",
            "Epoch [3/5], Loss: 0.0126\n",
            "Epoch [3/5], Loss: 0.0126\n",
            "Epoch [3/5], Loss: 0.0126\n",
            "Epoch [3/5], Loss: 0.0126\n",
            "Epoch [3/5], Loss: 0.0126\n",
            "Epoch [3/5], Loss: 0.0127\n",
            "Epoch [3/5], Loss: 0.0127\n",
            "Epoch [3/5], Loss: 0.0128\n",
            "Epoch [3/5], Loss: 0.0128\n",
            "Epoch [3/5], Loss: 0.0128\n",
            "Epoch [3/5], Loss: 0.0129\n",
            "Epoch [3/5], Loss: 0.0130\n",
            "Epoch [3/5], Loss: 0.0131\n",
            "Epoch [3/5], Loss: 0.0131\n",
            "Epoch [3/5], Loss: 0.0131\n",
            "Epoch [3/5], Loss: 0.0132\n",
            "Epoch [3/5], Loss: 0.0132\n",
            "Epoch [3/5], Loss: 0.0133\n",
            "Epoch [3/5], Loss: 0.0134\n",
            "Epoch [3/5], Loss: 0.0135\n",
            "Epoch [3/5], Loss: 0.0135\n",
            "Epoch [3/5], Loss: 0.0135\n",
            "Epoch [3/5], Loss: 0.0136\n",
            "Epoch [3/5], Loss: 0.0138\n",
            "Epoch [3/5], Loss: 0.0138\n",
            "Epoch [3/5], Loss: 0.0139\n",
            "Epoch [3/5], Loss: 0.0139\n",
            "Epoch [3/5], Loss: 0.0139\n",
            "Epoch [3/5], Loss: 0.0139\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0141\n",
            "Epoch [3/5], Loss: 0.0142\n",
            "Epoch [3/5], Loss: 0.0142\n",
            "Epoch [3/5], Loss: 0.0142\n",
            "Epoch [3/5], Loss: 0.0143\n",
            "Epoch [3/5], Loss: 0.0144\n",
            "Epoch [3/5], Loss: 0.0145\n",
            "Epoch [3/5], Loss: 0.0145\n",
            "Epoch [3/5], Loss: 0.0145\n",
            "Epoch [3/5], Loss: 0.0146\n",
            "Epoch [3/5], Loss: 0.0147\n",
            "Epoch [3/5], Loss: 0.0147\n",
            "Epoch [3/5], Loss: 0.0150\n",
            "Epoch [3/5], Loss: 0.0150\n",
            "Epoch [3/5], Loss: 0.0151\n",
            "Epoch [3/5], Loss: 0.0151\n",
            "Epoch [3/5], Loss: 0.0151\n",
            "Epoch [3/5], Loss: 0.0153\n",
            "Epoch [3/5], Loss: 0.0153\n",
            "Epoch [3/5], Loss: 0.0154\n",
            "Epoch [3/5], Loss: 0.0155\n",
            "Epoch [3/5], Loss: 0.0156\n",
            "Epoch [3/5], Loss: 0.0156\n",
            "Epoch [3/5], Loss: 0.0158\n",
            "Epoch [3/5], Loss: 0.0158\n",
            "Epoch [3/5], Loss: 0.0158\n",
            "Epoch [3/5], Loss: 0.0159\n",
            "Epoch [3/5], Loss: 0.0160\n",
            "Epoch [3/5], Loss: 0.0160\n",
            "Epoch [3/5], Loss: 0.0161\n",
            "Epoch [3/5], Loss: 0.0162\n",
            "Epoch [3/5], Loss: 0.0162\n",
            "Epoch [3/5], Loss: 0.0162\n",
            "Epoch [3/5], Loss: 0.0163\n",
            "Epoch [3/5], Loss: 0.0164\n",
            "Epoch [3/5], Loss: 0.0165\n",
            "Epoch [3/5], Loss: 0.0166\n",
            "Epoch [3/5], Loss: 0.0166\n",
            "Epoch [3/5], Loss: 0.0167\n",
            "Epoch [3/5], Loss: 0.0167\n",
            "Epoch [3/5], Loss: 0.0167\n",
            "Epoch [3/5], Loss: 0.0168\n",
            "Epoch [3/5], Loss: 0.0168\n",
            "Epoch [3/5], Loss: 0.0168\n",
            "Epoch [3/5], Loss: 0.0169\n",
            "Epoch [3/5], Loss: 0.0169\n",
            "Epoch [3/5], Loss: 0.0170\n",
            "Epoch [3/5], Loss: 0.0170\n",
            "Epoch [3/5], Loss: 0.0171\n",
            "Epoch [3/5], Loss: 0.0171\n",
            "Epoch [3/5], Loss: 0.0171\n",
            "Epoch [3/5], Loss: 0.0172\n",
            "Epoch [3/5], Loss: 0.0173\n",
            "Epoch [3/5], Loss: 0.0174\n",
            "Epoch [3/5], Loss: 0.0175\n",
            "Epoch [3/5], Loss: 0.0179\n",
            "Epoch [3/5], Loss: 0.0179\n",
            "Epoch [3/5], Loss: 0.0179\n",
            "Epoch [3/5], Loss: 0.0179\n",
            "Epoch [3/5], Loss: 0.0180\n",
            "Epoch [3/5], Loss: 0.0181\n",
            "Epoch [3/5], Loss: 0.0181\n",
            "Epoch [3/5], Loss: 0.0181\n",
            "Epoch [3/5], Loss: 0.0182\n",
            "Epoch [3/5], Loss: 0.0183\n",
            "Epoch [3/5], Loss: 0.0183\n",
            "Epoch [3/5], Loss: 0.0183\n",
            "Epoch [3/5], Loss: 0.0184\n",
            "Epoch [3/5], Loss: 0.0184\n",
            "Epoch [3/5], Loss: 0.0184\n",
            "Epoch [3/5], Loss: 0.0184\n",
            "Epoch [3/5], Loss: 0.0185\n",
            "Epoch [3/5], Loss: 0.0186\n",
            "Epoch [3/5], Loss: 0.0186\n",
            "Epoch [3/5], Loss: 0.0186\n",
            "Epoch [3/5], Loss: 0.0187\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0188\n",
            "Epoch [3/5], Loss: 0.0191\n",
            "Epoch [3/5], Loss: 0.0192\n",
            "Epoch [3/5], Loss: 0.0192\n",
            "Epoch [3/5], Loss: 0.0192\n",
            "Epoch [3/5], Loss: 0.0192\n",
            "Epoch [3/5], Loss: 0.0193\n",
            "Epoch [3/5], Loss: 0.0193\n",
            "Epoch [3/5], Loss: 0.0193\n",
            "Epoch [3/5], Loss: 0.0194\n",
            "Epoch [3/5], Loss: 0.0195\n",
            "Epoch [3/5], Loss: 0.0196\n",
            "Epoch [3/5], Loss: 0.0197\n",
            "Epoch [3/5], Loss: 0.0197\n",
            "Epoch [3/5], Loss: 0.0198\n",
            "Epoch [3/5], Loss: 0.0198\n",
            "Epoch [3/5], Loss: 0.0199\n",
            "Epoch [3/5], Loss: 0.0199\n",
            "Epoch [3/5], Loss: 0.0199\n",
            "Epoch [3/5], Loss: 0.0199\n",
            "Epoch [3/5], Loss: 0.0199\n",
            "Epoch [3/5], Loss: 0.0200\n",
            "Epoch [3/5], Loss: 0.0200\n",
            "Epoch [3/5], Loss: 0.0201\n",
            "Epoch [3/5], Loss: 0.0201\n",
            "Epoch [3/5], Loss: 0.0201\n",
            "Epoch [3/5], Loss: 0.0202\n",
            "Epoch [3/5], Loss: 0.0203\n",
            "Epoch [3/5], Loss: 0.0203\n",
            "Epoch [3/5], Loss: 0.0204\n",
            "Epoch [3/5], Loss: 0.0204\n",
            "Epoch [3/5], Loss: 0.0204\n",
            "Epoch [3/5], Loss: 0.0204\n",
            "Epoch [3/5], Loss: 0.0205\n",
            "Epoch [3/5], Loss: 0.0205\n",
            "Epoch [3/5], Loss: 0.0206\n",
            "Epoch [3/5], Loss: 0.0208\n",
            "Epoch [3/5], Loss: 0.0208\n",
            "Epoch [3/5], Loss: 0.0209\n",
            "Epoch [3/5], Loss: 0.0209\n",
            "Epoch [3/5], Loss: 0.0210\n",
            "Epoch [3/5], Loss: 0.0210\n",
            "Epoch [3/5], Loss: 0.0212\n",
            "Epoch [3/5], Loss: 0.0212\n",
            "Epoch [3/5], Loss: 0.0213\n",
            "Epoch [3/5], Loss: 0.0214\n",
            "Epoch [3/5], Loss: 0.0215\n",
            "Epoch [3/5], Loss: 0.0215\n",
            "Epoch [3/5], Loss: 0.0216\n",
            "Epoch [3/5], Loss: 0.0217\n",
            "Epoch [3/5], Loss: 0.0218\n",
            "Epoch [3/5], Loss: 0.0220\n",
            "Epoch [3/5], Loss: 0.0222\n",
            "Epoch [3/5], Loss: 0.0223\n",
            "Epoch [3/5], Loss: 0.0223\n",
            "Epoch [3/5], Loss: 0.0223\n",
            "Epoch [3/5], Loss: 0.0224\n",
            "Epoch [3/5], Loss: 0.0224\n",
            "Epoch [3/5], Loss: 0.0224\n",
            "Epoch [3/5], Loss: 0.0225\n",
            "Epoch [3/5], Loss: 0.0225\n",
            "Epoch [3/5], Loss: 0.0226\n",
            "Epoch [3/5], Loss: 0.0227\n",
            "Epoch [3/5], Loss: 0.0228\n",
            "Epoch [3/5], Loss: 0.0228\n",
            "Epoch [3/5], Loss: 0.0229\n",
            "Epoch [3/5], Loss: 0.0229\n",
            "Epoch [3/5], Loss: 0.0230\n",
            "Epoch [3/5], Loss: 0.0230\n",
            "Epoch [3/5], Loss: 0.0231\n",
            "Epoch [3/5], Loss: 0.0231\n",
            "Epoch [3/5], Loss: 0.0232\n",
            "Epoch [3/5], Loss: 0.0233\n",
            "Epoch [3/5], Loss: 0.0234\n",
            "Epoch [3/5], Loss: 0.0234\n",
            "Epoch [3/5], Loss: 0.0234\n",
            "Epoch [3/5], Loss: 0.0234\n",
            "Epoch [3/5], Loss: 0.0235\n",
            "Epoch [3/5], Loss: 0.0235\n",
            "Epoch [3/5], Loss: 0.0235\n",
            "Epoch [3/5], Loss: 0.0235\n",
            "Epoch [3/5], Loss: 0.0236\n",
            "Epoch [3/5], Loss: 0.0236\n",
            "Epoch [3/5], Loss: 0.0236\n",
            "Epoch [3/5], Loss: 0.0236\n",
            "Epoch [3/5], Loss: 0.0236\n",
            "Epoch [3/5], Loss: 0.0238\n",
            "Epoch [3/5], Loss: 0.0238\n",
            "Epoch [3/5], Loss: 0.0238\n",
            "Epoch [3/5], Loss: 0.0238\n",
            "Epoch [3/5], Loss: 0.0239\n",
            "Epoch [3/5], Loss: 0.0240\n",
            "Epoch [3/5], Loss: 0.0240\n",
            "Epoch [3/5], Loss: 0.0241\n",
            "Epoch [3/5], Loss: 0.0242\n",
            "Epoch [3/5], Loss: 0.0242\n",
            "Epoch [3/5], Loss: 0.0243\n",
            "Epoch [3/5], Loss: 0.0243\n",
            "Epoch [3/5], Loss: 0.0243\n",
            "Epoch [3/5], Loss: 0.0243\n",
            "Epoch [3/5], Loss: 0.0243\n",
            "Epoch [3/5], Loss: 0.0246\n",
            "Epoch [3/5], Loss: 0.0246\n",
            "Epoch [3/5], Loss: 0.0246\n",
            "Epoch [3/5], Loss: 0.0247\n",
            "Epoch [3/5], Loss: 0.0247\n",
            "Epoch [3/5], Loss: 0.0248\n",
            "Epoch [3/5], Loss: 0.0249\n",
            "Epoch [3/5], Loss: 0.0250\n",
            "Epoch [3/5], Loss: 0.0250\n",
            "Epoch [3/5], Loss: 0.0250\n",
            "Epoch [3/5], Loss: 0.0250\n",
            "Epoch [3/5], Loss: 0.0251\n",
            "Epoch [4/5], Loss: 0.0000\n",
            "Epoch [4/5], Loss: 0.0000\n",
            "Epoch [4/5], Loss: 0.0000\n",
            "Epoch [4/5], Loss: 0.0000\n",
            "Epoch [4/5], Loss: 0.0001\n",
            "Epoch [4/5], Loss: 0.0001\n",
            "Epoch [4/5], Loss: 0.0001\n",
            "Epoch [4/5], Loss: 0.0001\n",
            "Epoch [4/5], Loss: 0.0001\n",
            "Epoch [4/5], Loss: 0.0002\n",
            "Epoch [4/5], Loss: 0.0002\n",
            "Epoch [4/5], Loss: 0.0002\n",
            "Epoch [4/5], Loss: 0.0003\n",
            "Epoch [4/5], Loss: 0.0003\n",
            "Epoch [4/5], Loss: 0.0003\n",
            "Epoch [4/5], Loss: 0.0003\n",
            "Epoch [4/5], Loss: 0.0004\n",
            "Epoch [4/5], Loss: 0.0005\n",
            "Epoch [4/5], Loss: 0.0005\n",
            "Epoch [4/5], Loss: 0.0005\n",
            "Epoch [4/5], Loss: 0.0005\n",
            "Epoch [4/5], Loss: 0.0006\n",
            "Epoch [4/5], Loss: 0.0006\n",
            "Epoch [4/5], Loss: 0.0006\n",
            "Epoch [4/5], Loss: 0.0007\n",
            "Epoch [4/5], Loss: 0.0007\n",
            "Epoch [4/5], Loss: 0.0007\n",
            "Epoch [4/5], Loss: 0.0008\n",
            "Epoch [4/5], Loss: 0.0008\n",
            "Epoch [4/5], Loss: 0.0009\n",
            "Epoch [4/5], Loss: 0.0009\n",
            "Epoch [4/5], Loss: 0.0009\n",
            "Epoch [4/5], Loss: 0.0010\n",
            "Epoch [4/5], Loss: 0.0010\n",
            "Epoch [4/5], Loss: 0.0010\n",
            "Epoch [4/5], Loss: 0.0012\n",
            "Epoch [4/5], Loss: 0.0012\n",
            "Epoch [4/5], Loss: 0.0012\n",
            "Epoch [4/5], Loss: 0.0013\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0014\n",
            "Epoch [4/5], Loss: 0.0015\n",
            "Epoch [4/5], Loss: 0.0016\n",
            "Epoch [4/5], Loss: 0.0016\n",
            "Epoch [4/5], Loss: 0.0017\n",
            "Epoch [4/5], Loss: 0.0017\n",
            "Epoch [4/5], Loss: 0.0017\n",
            "Epoch [4/5], Loss: 0.0017\n",
            "Epoch [4/5], Loss: 0.0018\n",
            "Epoch [4/5], Loss: 0.0018\n",
            "Epoch [4/5], Loss: 0.0018\n",
            "Epoch [4/5], Loss: 0.0018\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0019\n",
            "Epoch [4/5], Loss: 0.0020\n",
            "Epoch [4/5], Loss: 0.0020\n",
            "Epoch [4/5], Loss: 0.0020\n",
            "Epoch [4/5], Loss: 0.0021\n",
            "Epoch [4/5], Loss: 0.0021\n",
            "Epoch [4/5], Loss: 0.0021\n",
            "Epoch [4/5], Loss: 0.0022\n",
            "Epoch [4/5], Loss: 0.0022\n",
            "Epoch [4/5], Loss: 0.0023\n",
            "Epoch [4/5], Loss: 0.0023\n",
            "Epoch [4/5], Loss: 0.0023\n",
            "Epoch [4/5], Loss: 0.0023\n",
            "Epoch [4/5], Loss: 0.0023\n",
            "Epoch [4/5], Loss: 0.0024\n",
            "Epoch [4/5], Loss: 0.0024\n",
            "Epoch [4/5], Loss: 0.0024\n",
            "Epoch [4/5], Loss: 0.0024\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [4/5], Loss: 0.0027\n",
            "Epoch [4/5], Loss: 0.0027\n",
            "Epoch [4/5], Loss: 0.0027\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0028\n",
            "Epoch [4/5], Loss: 0.0029\n",
            "Epoch [4/5], Loss: 0.0029\n",
            "Epoch [4/5], Loss: 0.0030\n",
            "Epoch [4/5], Loss: 0.0030\n",
            "Epoch [4/5], Loss: 0.0031\n",
            "Epoch [4/5], Loss: 0.0031\n",
            "Epoch [4/5], Loss: 0.0031\n",
            "Epoch [4/5], Loss: 0.0031\n",
            "Epoch [4/5], Loss: 0.0032\n",
            "Epoch [4/5], Loss: 0.0032\n",
            "Epoch [4/5], Loss: 0.0032\n",
            "Epoch [4/5], Loss: 0.0033\n",
            "Epoch [4/5], Loss: 0.0033\n",
            "Epoch [4/5], Loss: 0.0033\n",
            "Epoch [4/5], Loss: 0.0033\n",
            "Epoch [4/5], Loss: 0.0034\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0035\n",
            "Epoch [4/5], Loss: 0.0036\n",
            "Epoch [4/5], Loss: 0.0036\n",
            "Epoch [4/5], Loss: 0.0037\n",
            "Epoch [4/5], Loss: 0.0037\n",
            "Epoch [4/5], Loss: 0.0038\n",
            "Epoch [4/5], Loss: 0.0038\n",
            "Epoch [4/5], Loss: 0.0038\n",
            "Epoch [4/5], Loss: 0.0038\n",
            "Epoch [4/5], Loss: 0.0039\n",
            "Epoch [4/5], Loss: 0.0039\n",
            "Epoch [4/5], Loss: 0.0040\n",
            "Epoch [4/5], Loss: 0.0040\n",
            "Epoch [4/5], Loss: 0.0040\n",
            "Epoch [4/5], Loss: 0.0041\n",
            "Epoch [4/5], Loss: 0.0041\n",
            "Epoch [4/5], Loss: 0.0041\n",
            "Epoch [4/5], Loss: 0.0041\n",
            "Epoch [4/5], Loss: 0.0041\n",
            "Epoch [4/5], Loss: 0.0042\n",
            "Epoch [4/5], Loss: 0.0043\n",
            "Epoch [4/5], Loss: 0.0043\n",
            "Epoch [4/5], Loss: 0.0043\n",
            "Epoch [4/5], Loss: 0.0045\n",
            "Epoch [4/5], Loss: 0.0045\n",
            "Epoch [4/5], Loss: 0.0045\n",
            "Epoch [4/5], Loss: 0.0045\n",
            "Epoch [4/5], Loss: 0.0045\n",
            "Epoch [4/5], Loss: 0.0046\n",
            "Epoch [4/5], Loss: 0.0046\n",
            "Epoch [4/5], Loss: 0.0046\n",
            "Epoch [4/5], Loss: 0.0046\n",
            "Epoch [4/5], Loss: 0.0047\n",
            "Epoch [4/5], Loss: 0.0048\n",
            "Epoch [4/5], Loss: 0.0048\n",
            "Epoch [4/5], Loss: 0.0048\n",
            "Epoch [4/5], Loss: 0.0049\n",
            "Epoch [4/5], Loss: 0.0049\n",
            "Epoch [4/5], Loss: 0.0049\n",
            "Epoch [4/5], Loss: 0.0049\n",
            "Epoch [4/5], Loss: 0.0049\n",
            "Epoch [4/5], Loss: 0.0051\n",
            "Epoch [4/5], Loss: 0.0051\n",
            "Epoch [4/5], Loss: 0.0051\n",
            "Epoch [4/5], Loss: 0.0052\n",
            "Epoch [4/5], Loss: 0.0052\n",
            "Epoch [4/5], Loss: 0.0052\n",
            "Epoch [4/5], Loss: 0.0053\n",
            "Epoch [4/5], Loss: 0.0053\n",
            "Epoch [4/5], Loss: 0.0053\n",
            "Epoch [4/5], Loss: 0.0054\n",
            "Epoch [4/5], Loss: 0.0054\n",
            "Epoch [4/5], Loss: 0.0054\n",
            "Epoch [4/5], Loss: 0.0054\n",
            "Epoch [4/5], Loss: 0.0055\n",
            "Epoch [4/5], Loss: 0.0055\n",
            "Epoch [4/5], Loss: 0.0055\n",
            "Epoch [4/5], Loss: 0.0056\n",
            "Epoch [4/5], Loss: 0.0056\n",
            "Epoch [4/5], Loss: 0.0057\n",
            "Epoch [4/5], Loss: 0.0057\n",
            "Epoch [4/5], Loss: 0.0058\n",
            "Epoch [4/5], Loss: 0.0058\n",
            "Epoch [4/5], Loss: 0.0058\n",
            "Epoch [4/5], Loss: 0.0059\n",
            "Epoch [4/5], Loss: 0.0059\n",
            "Epoch [4/5], Loss: 0.0061\n",
            "Epoch [4/5], Loss: 0.0061\n",
            "Epoch [4/5], Loss: 0.0061\n",
            "Epoch [4/5], Loss: 0.0061\n",
            "Epoch [4/5], Loss: 0.0061\n",
            "Epoch [4/5], Loss: 0.0062\n",
            "Epoch [4/5], Loss: 0.0062\n",
            "Epoch [4/5], Loss: 0.0062\n",
            "Epoch [4/5], Loss: 0.0062\n",
            "Epoch [4/5], Loss: 0.0063\n",
            "Epoch [4/5], Loss: 0.0063\n",
            "Epoch [4/5], Loss: 0.0064\n",
            "Epoch [4/5], Loss: 0.0064\n",
            "Epoch [4/5], Loss: 0.0064\n",
            "Epoch [4/5], Loss: 0.0064\n",
            "Epoch [4/5], Loss: 0.0064\n",
            "Epoch [4/5], Loss: 0.0065\n",
            "Epoch [4/5], Loss: 0.0066\n",
            "Epoch [4/5], Loss: 0.0066\n",
            "Epoch [4/5], Loss: 0.0066\n",
            "Epoch [4/5], Loss: 0.0066\n",
            "Epoch [4/5], Loss: 0.0067\n",
            "Epoch [4/5], Loss: 0.0067\n",
            "Epoch [4/5], Loss: 0.0068\n",
            "Epoch [4/5], Loss: 0.0069\n",
            "Epoch [4/5], Loss: 0.0069\n",
            "Epoch [4/5], Loss: 0.0070\n",
            "Epoch [4/5], Loss: 0.0070\n",
            "Epoch [4/5], Loss: 0.0071\n",
            "Epoch [4/5], Loss: 0.0071\n",
            "Epoch [4/5], Loss: 0.0071\n",
            "Epoch [4/5], Loss: 0.0071\n",
            "Epoch [4/5], Loss: 0.0072\n",
            "Epoch [4/5], Loss: 0.0072\n",
            "Epoch [4/5], Loss: 0.0073\n",
            "Epoch [4/5], Loss: 0.0073\n",
            "Epoch [4/5], Loss: 0.0073\n",
            "Epoch [4/5], Loss: 0.0073\n",
            "Epoch [4/5], Loss: 0.0073\n",
            "Epoch [4/5], Loss: 0.0074\n",
            "Epoch [4/5], Loss: 0.0074\n",
            "Epoch [4/5], Loss: 0.0075\n",
            "Epoch [4/5], Loss: 0.0075\n",
            "Epoch [4/5], Loss: 0.0075\n",
            "Epoch [4/5], Loss: 0.0077\n",
            "Epoch [4/5], Loss: 0.0077\n",
            "Epoch [4/5], Loss: 0.0078\n",
            "Epoch [4/5], Loss: 0.0078\n",
            "Epoch [4/5], Loss: 0.0078\n",
            "Epoch [4/5], Loss: 0.0079\n",
            "Epoch [4/5], Loss: 0.0079\n",
            "Epoch [4/5], Loss: 0.0080\n",
            "Epoch [4/5], Loss: 0.0080\n",
            "Epoch [4/5], Loss: 0.0081\n",
            "Epoch [4/5], Loss: 0.0081\n",
            "Epoch [4/5], Loss: 0.0082\n",
            "Epoch [4/5], Loss: 0.0082\n",
            "Epoch [4/5], Loss: 0.0082\n",
            "Epoch [4/5], Loss: 0.0083\n",
            "Epoch [4/5], Loss: 0.0083\n",
            "Epoch [4/5], Loss: 0.0083\n",
            "Epoch [4/5], Loss: 0.0083\n",
            "Epoch [4/5], Loss: 0.0084\n",
            "Epoch [4/5], Loss: 0.0085\n",
            "Epoch [4/5], Loss: 0.0085\n",
            "Epoch [4/5], Loss: 0.0086\n",
            "Epoch [4/5], Loss: 0.0086\n",
            "Epoch [4/5], Loss: 0.0086\n",
            "Epoch [4/5], Loss: 0.0087\n",
            "Epoch [4/5], Loss: 0.0087\n",
            "Epoch [4/5], Loss: 0.0087\n",
            "Epoch [4/5], Loss: 0.0088\n",
            "Epoch [4/5], Loss: 0.0089\n",
            "Epoch [4/5], Loss: 0.0089\n",
            "Epoch [4/5], Loss: 0.0089\n",
            "Epoch [4/5], Loss: 0.0090\n",
            "Epoch [4/5], Loss: 0.0090\n",
            "Epoch [4/5], Loss: 0.0090\n",
            "Epoch [4/5], Loss: 0.0092\n",
            "Epoch [4/5], Loss: 0.0092\n",
            "Epoch [4/5], Loss: 0.0092\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [4/5], Loss: 0.0093\n",
            "Epoch [4/5], Loss: 0.0094\n",
            "Epoch [4/5], Loss: 0.0095\n",
            "Epoch [4/5], Loss: 0.0095\n",
            "Epoch [4/5], Loss: 0.0095\n",
            "Epoch [4/5], Loss: 0.0095\n",
            "Epoch [4/5], Loss: 0.0095\n",
            "Epoch [4/5], Loss: 0.0096\n",
            "Epoch [4/5], Loss: 0.0096\n",
            "Epoch [4/5], Loss: 0.0096\n",
            "Epoch [4/5], Loss: 0.0096\n",
            "Epoch [4/5], Loss: 0.0096\n",
            "Epoch [4/5], Loss: 0.0097\n",
            "Epoch [4/5], Loss: 0.0097\n",
            "Epoch [4/5], Loss: 0.0097\n",
            "Epoch [4/5], Loss: 0.0097\n",
            "Epoch [4/5], Loss: 0.0098\n",
            "Epoch [4/5], Loss: 0.0099\n",
            "Epoch [4/5], Loss: 0.0099\n",
            "Epoch [4/5], Loss: 0.0099\n",
            "Epoch [4/5], Loss: 0.0100\n",
            "Epoch [4/5], Loss: 0.0100\n",
            "Epoch [4/5], Loss: 0.0100\n",
            "Epoch [4/5], Loss: 0.0100\n",
            "Epoch [4/5], Loss: 0.0100\n",
            "Epoch [4/5], Loss: 0.0102\n",
            "Epoch [4/5], Loss: 0.0102\n",
            "Epoch [4/5], Loss: 0.0103\n",
            "Epoch [4/5], Loss: 0.0103\n",
            "Epoch [4/5], Loss: 0.0103\n",
            "Epoch [4/5], Loss: 0.0103\n",
            "Epoch [4/5], Loss: 0.0103\n",
            "Epoch [4/5], Loss: 0.0104\n",
            "Epoch [4/5], Loss: 0.0104\n",
            "Epoch [4/5], Loss: 0.0105\n",
            "Epoch [4/5], Loss: 0.0105\n",
            "Epoch [4/5], Loss: 0.0106\n",
            "Epoch [4/5], Loss: 0.0107\n",
            "Epoch [4/5], Loss: 0.0107\n",
            "Epoch [4/5], Loss: 0.0108\n",
            "Epoch [4/5], Loss: 0.0108\n",
            "Epoch [4/5], Loss: 0.0109\n",
            "Epoch [4/5], Loss: 0.0109\n",
            "Epoch [4/5], Loss: 0.0110\n",
            "Epoch [4/5], Loss: 0.0110\n",
            "Epoch [4/5], Loss: 0.0111\n",
            "Epoch [4/5], Loss: 0.0111\n",
            "Epoch [4/5], Loss: 0.0111\n",
            "Epoch [4/5], Loss: 0.0112\n",
            "Epoch [4/5], Loss: 0.0112\n",
            "Epoch [4/5], Loss: 0.0112\n",
            "Epoch [4/5], Loss: 0.0112\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0113\n",
            "Epoch [4/5], Loss: 0.0114\n",
            "Epoch [4/5], Loss: 0.0114\n",
            "Epoch [4/5], Loss: 0.0114\n",
            "Epoch [4/5], Loss: 0.0114\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0115\n",
            "Epoch [4/5], Loss: 0.0116\n",
            "Epoch [4/5], Loss: 0.0116\n",
            "Epoch [4/5], Loss: 0.0116\n",
            "Epoch [4/5], Loss: 0.0117\n",
            "Epoch [4/5], Loss: 0.0117\n",
            "Epoch [4/5], Loss: 0.0117\n",
            "Epoch [4/5], Loss: 0.0117\n",
            "Epoch [4/5], Loss: 0.0117\n",
            "Epoch [4/5], Loss: 0.0118\n",
            "Epoch [4/5], Loss: 0.0118\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0119\n",
            "Epoch [4/5], Loss: 0.0120\n",
            "Epoch [4/5], Loss: 0.0120\n",
            "Epoch [4/5], Loss: 0.0120\n",
            "Epoch [4/5], Loss: 0.0121\n",
            "Epoch [4/5], Loss: 0.0122\n",
            "Epoch [4/5], Loss: 0.0122\n",
            "Epoch [4/5], Loss: 0.0122\n",
            "Epoch [4/5], Loss: 0.0122\n",
            "Epoch [4/5], Loss: 0.0122\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0123\n",
            "Epoch [4/5], Loss: 0.0124\n",
            "Epoch [4/5], Loss: 0.0124\n",
            "Epoch [4/5], Loss: 0.0124\n",
            "Epoch [4/5], Loss: 0.0124\n",
            "Epoch [4/5], Loss: 0.0124\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0125\n",
            "Epoch [4/5], Loss: 0.0126\n",
            "Epoch [4/5], Loss: 0.0126\n",
            "Epoch [4/5], Loss: 0.0127\n",
            "Epoch [4/5], Loss: 0.0127\n",
            "Epoch [4/5], Loss: 0.0128\n",
            "Epoch [4/5], Loss: 0.0128\n",
            "Epoch [4/5], Loss: 0.0128\n",
            "Epoch [4/5], Loss: 0.0128\n",
            "Epoch [4/5], Loss: 0.0129\n",
            "Epoch [4/5], Loss: 0.0129\n",
            "Epoch [4/5], Loss: 0.0129\n",
            "Epoch [4/5], Loss: 0.0129\n",
            "Epoch [4/5], Loss: 0.0130\n",
            "Epoch [4/5], Loss: 0.0130\n",
            "Epoch [4/5], Loss: 0.0130\n",
            "Epoch [4/5], Loss: 0.0131\n",
            "Epoch [4/5], Loss: 0.0132\n",
            "Epoch [4/5], Loss: 0.0132\n",
            "Epoch [4/5], Loss: 0.0132\n",
            "Epoch [4/5], Loss: 0.0132\n",
            "Epoch [4/5], Loss: 0.0132\n",
            "Epoch [4/5], Loss: 0.0134\n",
            "Epoch [4/5], Loss: 0.0134\n",
            "Epoch [4/5], Loss: 0.0134\n",
            "Epoch [4/5], Loss: 0.0134\n",
            "Epoch [4/5], Loss: 0.0134\n",
            "Epoch [4/5], Loss: 0.0135\n",
            "Epoch [4/5], Loss: 0.0135\n",
            "Epoch [4/5], Loss: 0.0135\n",
            "Epoch [4/5], Loss: 0.0135\n",
            "Epoch [4/5], Loss: 0.0138\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0139\n",
            "Epoch [4/5], Loss: 0.0140\n",
            "Epoch [4/5], Loss: 0.0140\n",
            "Epoch [4/5], Loss: 0.0141\n",
            "Epoch [4/5], Loss: 0.0142\n",
            "Epoch [4/5], Loss: 0.0143\n",
            "Epoch [4/5], Loss: 0.0143\n",
            "Epoch [4/5], Loss: 0.0143\n",
            "Epoch [4/5], Loss: 0.0143\n",
            "Epoch [4/5], Loss: 0.0145\n",
            "Epoch [4/5], Loss: 0.0146\n",
            "Epoch [4/5], Loss: 0.0146\n",
            "Epoch [4/5], Loss: 0.0147\n",
            "Epoch [4/5], Loss: 0.0147\n",
            "Epoch [4/5], Loss: 0.0147\n",
            "Epoch [4/5], Loss: 0.0147\n",
            "Epoch [4/5], Loss: 0.0148\n",
            "Epoch [4/5], Loss: 0.0148\n",
            "Epoch [4/5], Loss: 0.0148\n",
            "Epoch [4/5], Loss: 0.0149\n",
            "Epoch [4/5], Loss: 0.0149\n",
            "Epoch [4/5], Loss: 0.0149\n",
            "Epoch [4/5], Loss: 0.0149\n",
            "Epoch [4/5], Loss: 0.0149\n",
            "Epoch [4/5], Loss: 0.0150\n",
            "Epoch [4/5], Loss: 0.0150\n",
            "Epoch [4/5], Loss: 0.0150\n",
            "Epoch [4/5], Loss: 0.0151\n",
            "Epoch [4/5], Loss: 0.0152\n",
            "Epoch [4/5], Loss: 0.0152\n",
            "Epoch [4/5], Loss: 0.0152\n",
            "Epoch [4/5], Loss: 0.0152\n",
            "Epoch [4/5], Loss: 0.0153\n",
            "Epoch [4/5], Loss: 0.0153\n",
            "Epoch [4/5], Loss: 0.0154\n",
            "Epoch [4/5], Loss: 0.0154\n",
            "Epoch [4/5], Loss: 0.0155\n",
            "Epoch [4/5], Loss: 0.0156\n",
            "Epoch [4/5], Loss: 0.0156\n",
            "Epoch [4/5], Loss: 0.0156\n",
            "Epoch [4/5], Loss: 0.0156\n",
            "Epoch [4/5], Loss: 0.0158\n",
            "Epoch [4/5], Loss: 0.0160\n",
            "Epoch [4/5], Loss: 0.0160\n",
            "Epoch [4/5], Loss: 0.0160\n",
            "Epoch [4/5], Loss: 0.0160\n",
            "Epoch [4/5], Loss: 0.0160\n",
            "Epoch [4/5], Loss: 0.0161\n",
            "Epoch [4/5], Loss: 0.0161\n",
            "Epoch [4/5], Loss: 0.0161\n",
            "Epoch [4/5], Loss: 0.0162\n",
            "Epoch [4/5], Loss: 0.0162\n",
            "Epoch [4/5], Loss: 0.0162\n",
            "Epoch [5/5], Loss: 0.0000\n",
            "Epoch [5/5], Loss: 0.0000\n",
            "Epoch [5/5], Loss: 0.0001\n",
            "Epoch [5/5], Loss: 0.0001\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0002\n",
            "Epoch [5/5], Loss: 0.0003\n",
            "Epoch [5/5], Loss: 0.0003\n",
            "Epoch [5/5], Loss: 0.0003\n",
            "Epoch [5/5], Loss: 0.0003\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0004\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0005\n",
            "Epoch [5/5], Loss: 0.0006\n",
            "Epoch [5/5], Loss: 0.0006\n",
            "Epoch [5/5], Loss: 0.0006\n",
            "Epoch [5/5], Loss: 0.0006\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0007\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0008\n",
            "Epoch [5/5], Loss: 0.0010\n",
            "Epoch [5/5], Loss: 0.0010\n",
            "Epoch [5/5], Loss: 0.0010\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Epoch [5/5], Loss: 0.0012\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0013\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0014\n",
            "Epoch [5/5], Loss: 0.0015\n",
            "Epoch [5/5], Loss: 0.0015\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0018\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0019\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0020\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0021\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0022\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0023\n",
            "Epoch [5/5], Loss: 0.0024\n",
            "Epoch [5/5], Loss: 0.0024\n",
            "Epoch [5/5], Loss: 0.0024\n",
            "Epoch [5/5], Loss: 0.0024\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0025\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0027\n",
            "Epoch [5/5], Loss: 0.0027\n",
            "Epoch [5/5], Loss: 0.0027\n",
            "Epoch [5/5], Loss: 0.0027\n",
            "Epoch [5/5], Loss: 0.0027\n",
            "Epoch [5/5], Loss: 0.0028\n",
            "Epoch [5/5], Loss: 0.0028\n",
            "Epoch [5/5], Loss: 0.0028\n",
            "Epoch [5/5], Loss: 0.0029\n",
            "Epoch [5/5], Loss: 0.0029\n",
            "Epoch [5/5], Loss: 0.0029\n",
            "Epoch [5/5], Loss: 0.0029\n",
            "Epoch [5/5], Loss: 0.0030\n",
            "Epoch [5/5], Loss: 0.0030\n",
            "Epoch [5/5], Loss: 0.0031\n",
            "Epoch [5/5], Loss: 0.0031\n",
            "Epoch [5/5], Loss: 0.0031\n",
            "Epoch [5/5], Loss: 0.0031\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0033\n",
            "Epoch [5/5], Loss: 0.0034\n",
            "Epoch [5/5], Loss: 0.0034\n",
            "Epoch [5/5], Loss: 0.0034\n",
            "Epoch [5/5], Loss: 0.0034\n",
            "Epoch [5/5], Loss: 0.0035\n",
            "Epoch [5/5], Loss: 0.0035\n",
            "Epoch [5/5], Loss: 0.0035\n",
            "Epoch [5/5], Loss: 0.0035\n",
            "Epoch [5/5], Loss: 0.0036\n",
            "Epoch [5/5], Loss: 0.0036\n",
            "Epoch [5/5], Loss: 0.0036\n",
            "Epoch [5/5], Loss: 0.0037\n",
            "Epoch [5/5], Loss: 0.0037\n",
            "Epoch [5/5], Loss: 0.0037\n",
            "Epoch [5/5], Loss: 0.0038\n",
            "Epoch [5/5], Loss: 0.0038\n",
            "Epoch [5/5], Loss: 0.0038\n",
            "Epoch [5/5], Loss: 0.0038\n",
            "Epoch [5/5], Loss: 0.0038\n",
            "Epoch [5/5], Loss: 0.0039\n",
            "Epoch [5/5], Loss: 0.0039\n",
            "Epoch [5/5], Loss: 0.0039\n",
            "Epoch [5/5], Loss: 0.0039\n",
            "Epoch [5/5], Loss: 0.0039\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0040\n",
            "Epoch [5/5], Loss: 0.0042\n",
            "Epoch [5/5], Loss: 0.0042\n",
            "Epoch [5/5], Loss: 0.0042\n",
            "Epoch [5/5], Loss: 0.0042\n",
            "Epoch [5/5], Loss: 0.0043\n",
            "Epoch [5/5], Loss: 0.0043\n",
            "Epoch [5/5], Loss: 0.0043\n",
            "Epoch [5/5], Loss: 0.0043\n",
            "Epoch [5/5], Loss: 0.0043\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0044\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0045\n",
            "Epoch [5/5], Loss: 0.0046\n",
            "Epoch [5/5], Loss: 0.0047\n",
            "Epoch [5/5], Loss: 0.0047\n",
            "Epoch [5/5], Loss: 0.0047\n",
            "Epoch [5/5], Loss: 0.0047\n",
            "Epoch [5/5], Loss: 0.0048\n",
            "Epoch [5/5], Loss: 0.0048\n",
            "Epoch [5/5], Loss: 0.0048\n",
            "Epoch [5/5], Loss: 0.0048\n",
            "Epoch [5/5], Loss: 0.0049\n",
            "Epoch [5/5], Loss: 0.0049\n",
            "Epoch [5/5], Loss: 0.0049\n",
            "Epoch [5/5], Loss: 0.0050\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0051\n",
            "Epoch [5/5], Loss: 0.0052\n",
            "Epoch [5/5], Loss: 0.0052\n",
            "Epoch [5/5], Loss: 0.0052\n",
            "Epoch [5/5], Loss: 0.0053\n",
            "Epoch [5/5], Loss: 0.0053\n",
            "Epoch [5/5], Loss: 0.0053\n",
            "Epoch [5/5], Loss: 0.0054\n",
            "Epoch [5/5], Loss: 0.0054\n",
            "Epoch [5/5], Loss: 0.0054\n",
            "Epoch [5/5], Loss: 0.0054\n",
            "Epoch [5/5], Loss: 0.0055\n",
            "Epoch [5/5], Loss: 0.0055\n",
            "Epoch [5/5], Loss: 0.0056\n",
            "Epoch [5/5], Loss: 0.0056\n",
            "Epoch [5/5], Loss: 0.0056\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0057\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0058\n",
            "Epoch [5/5], Loss: 0.0059\n",
            "Epoch [5/5], Loss: 0.0059\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0060\n",
            "Epoch [5/5], Loss: 0.0061\n",
            "Epoch [5/5], Loss: 0.0062\n",
            "Epoch [5/5], Loss: 0.0062\n",
            "Epoch [5/5], Loss: 0.0062\n",
            "Epoch [5/5], Loss: 0.0063\n",
            "Epoch [5/5], Loss: 0.0063\n",
            "Epoch [5/5], Loss: 0.0063\n",
            "Epoch [5/5], Loss: 0.0064\n",
            "Epoch [5/5], Loss: 0.0064\n",
            "Epoch [5/5], Loss: 0.0064\n",
            "Epoch [5/5], Loss: 0.0064\n",
            "Epoch [5/5], Loss: 0.0065\n",
            "Epoch [5/5], Loss: 0.0065\n",
            "Epoch [5/5], Loss: 0.0066\n",
            "Epoch [5/5], Loss: 0.0066\n",
            "Epoch [5/5], Loss: 0.0067\n",
            "Epoch [5/5], Loss: 0.0068\n",
            "Epoch [5/5], Loss: 0.0068\n",
            "Epoch [5/5], Loss: 0.0068\n",
            "Epoch [5/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.0069\n",
            "Epoch [5/5], Loss: 0.0070\n",
            "Epoch [5/5], Loss: 0.0071\n",
            "Epoch [5/5], Loss: 0.0071\n",
            "Epoch [5/5], Loss: 0.0071\n",
            "Epoch [5/5], Loss: 0.0071\n",
            "Epoch [5/5], Loss: 0.0072\n",
            "Epoch [5/5], Loss: 0.0072\n",
            "Epoch [5/5], Loss: 0.0072\n",
            "Epoch [5/5], Loss: 0.0072\n",
            "Epoch [5/5], Loss: 0.0073\n",
            "Epoch [5/5], Loss: 0.0073\n",
            "Epoch [5/5], Loss: 0.0074\n",
            "Epoch [5/5], Loss: 0.0074\n",
            "Epoch [5/5], Loss: 0.0074\n",
            "Epoch [5/5], Loss: 0.0075\n",
            "Epoch [5/5], Loss: 0.0076\n",
            "Epoch [5/5], Loss: 0.0076\n",
            "Epoch [5/5], Loss: 0.0077\n",
            "Epoch [5/5], Loss: 0.0078\n",
            "Epoch [5/5], Loss: 0.0078\n",
            "Epoch [5/5], Loss: 0.0078\n",
            "Epoch [5/5], Loss: 0.0078\n",
            "Epoch [5/5], Loss: 0.0078\n",
            "Epoch [5/5], Loss: 0.0079\n",
            "Epoch [5/5], Loss: 0.0079\n",
            "Epoch [5/5], Loss: 0.0079\n",
            "Epoch [5/5], Loss: 0.0079\n",
            "Epoch [5/5], Loss: 0.0079\n",
            "Epoch [5/5], Loss: 0.0080\n",
            "Epoch [5/5], Loss: 0.0080\n",
            "Epoch [5/5], Loss: 0.0080\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0081\n",
            "Epoch [5/5], Loss: 0.0082\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0083\n",
            "Epoch [5/5], Loss: 0.0084\n",
            "Epoch [5/5], Loss: 0.0084\n",
            "Epoch [5/5], Loss: 0.0084\n",
            "Epoch [5/5], Loss: 0.0084\n",
            "Epoch [5/5], Loss: 0.0084\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0085\n",
            "Epoch [5/5], Loss: 0.0086\n",
            "Epoch [5/5], Loss: 0.0086\n",
            "Epoch [5/5], Loss: 0.0087\n",
            "Epoch [5/5], Loss: 0.0088\n",
            "Epoch [5/5], Loss: 0.0088\n",
            "Epoch [5/5], Loss: 0.0088\n",
            "Epoch [5/5], Loss: 0.0088\n",
            "Epoch [5/5], Loss: 0.0089\n",
            "Epoch [5/5], Loss: 0.0089\n",
            "Epoch [5/5], Loss: 0.0089\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0090\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0091\n",
            "Epoch [5/5], Loss: 0.0092\n",
            "Epoch [5/5], Loss: 0.0092\n",
            "Epoch [5/5], Loss: 0.0092\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0093\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0094\n",
            "Epoch [5/5], Loss: 0.0095\n",
            "Epoch [5/5], Loss: 0.0095\n",
            "Epoch [5/5], Loss: 0.0095\n",
            "Epoch [5/5], Loss: 0.0095\n",
            "Epoch [5/5], Loss: 0.0096\n",
            "Epoch [5/5], Loss: 0.0096\n",
            "Epoch [5/5], Loss: 0.0096\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0097\n",
            "Epoch [5/5], Loss: 0.0098\n",
            "Epoch [5/5], Loss: 0.0098\n",
            "Epoch [5/5], Loss: 0.0098\n",
            "Epoch [5/5], Loss: 0.0098\n",
            "Epoch [5/5], Loss: 0.0098\n",
            "Epoch [5/5], Loss: 0.0099\n",
            "Epoch [5/5], Loss: 0.0099\n",
            "Epoch [5/5], Loss: 0.0100\n",
            "Epoch [5/5], Loss: 0.0100\n",
            "Epoch [5/5], Loss: 0.0100\n",
            "Epoch [5/5], Loss: 0.0100\n",
            "Epoch [5/5], Loss: 0.0101\n",
            "Epoch [5/5], Loss: 0.0101\n",
            "Epoch [5/5], Loss: 0.0101\n",
            "Epoch [5/5], Loss: 0.0102\n",
            "Epoch [5/5], Loss: 0.0102\n",
            "Epoch [5/5], Loss: 0.0102\n",
            "Epoch [5/5], Loss: 0.0102\n",
            "Epoch [5/5], Loss: 0.0103\n",
            "Epoch [5/5], Loss: 0.0104\n",
            "Epoch [5/5], Loss: 0.0104\n",
            "Epoch [5/5], Loss: 0.0104\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0105\n",
            "Epoch [5/5], Loss: 0.0106\n",
            "Epoch [5/5], Loss: 0.0106\n",
            "Epoch [5/5], Loss: 0.0107\n",
            "Epoch [5/5], Loss: 0.0108\n",
            "Epoch [5/5], Loss: 0.0108\n",
            "Epoch [5/5], Loss: 0.0108\n",
            "Epoch [5/5], Loss: 0.0108\n",
            "Epoch [5/5], Loss: 0.0108\n",
            "Epoch [5/5], Loss: 0.0109\n",
            "Epoch [5/5], Loss: 0.0109\n",
            "Epoch [5/5], Loss: 0.0109\n",
            "Epoch [5/5], Loss: 0.0109\n",
            "Test Accuracy: 0.9853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "0dcaf871",
        "outputId": "13771ddc-b6ab-4dbe-f258-3ec5564d6c5b"
      },
      "source": [
        "# Q9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "#  Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#  Data Preprocessing using ImageDataGenerator\n",
        "train_dir = './data/train'  # Corrected path to your training dataset folder\n",
        "val_dir = './data/val'      # Corrected path to your validation dataset folder\n",
        "\n",
        "# Image augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Only rescale for validation\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(\"Class indices:\", train_generator.class_indices)\n",
        "\n",
        "#  CNN Model Definition\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#  Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "#  Save the Model\n",
        "model.save(\"bird_classifier_model.h5\")\n",
        "\n",
        "#  Predict on Uploaded Image\n",
        "def predict_image(img_path, model, target_size=(128,128)):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)/255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    preds = model.predict(img_array)\n",
        "    class_index = np.argmax(preds, axis=1)[0]\n",
        "\n",
        "    # Map index to class name\n",
        "    class_labels = {v:k for k,v in train_generator.class_indices.items()}\n",
        "    return class_labels[class_index]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 images belonging to 2 classes.\n",
            "Found 1 images belonging to 2 classes.\n",
            "Class indices: {'class_a': 0, 'class_b': 1}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,898\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,898</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,898\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,898</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0000e+00 - loss: 0.7472 - val_accuracy: 1.0000 - val_loss: 0.2663\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.2866 - val_accuracy: 1.0000 - val_loss: 0.0238\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0386 - val_accuracy: 1.0000 - val_loss: 2.8582e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0397 - val_accuracy: 1.0000 - val_loss: 3.5763e-07\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 1.7808e-04 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}